{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKQ4bH7qMGrA"
      },
      "source": [
        "# Making the Most of your Colab Subscription\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMMqmdiYMkvi"
      },
      "source": [
        "## Faster GPUs\n",
        "\n",
        "Users who have purchased one of Colab's paid plans have access to premium GPUs. You can upgrade your notebook's GPU settings in `Runtime > Change runtime type` in the menu to enable Premium accelerator. Subject to availability, selecting a premium GPU may grant you access to a V100 or A100 Nvidia GPU.\n",
        "\n",
        "The free of charge version of Colab grants access to Nvidia's T4 GPUs subject to quota restrictions and availability.\n",
        "\n",
        "You can see what GPU you've been assigned at any time by executing the following cell. If the execution result of running the code cell below is \"Not connected to a GPU\", you can change the runtime by going to `Runtime > Change runtime type` in the menu to enable a GPU accelerator, and then re-execute the code cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23TOba33L4qf",
        "outputId": "d3278dac-c363-457d-9dfb-3d656ca182c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Feb 20 17:37:26 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0    50W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sa-IrJS1aRVJ"
      },
      "source": [
        "In order to use a GPU with your notebook, select the `Runtime > Change runtime type` menu, and then set the hardware accelerator dropdown to GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65MSuHKqNeBZ"
      },
      "source": [
        "## More memory\n",
        "\n",
        "Users who have purchased one of Colab's paid plans have access to high-memory VMs when they are available.\n",
        "\n",
        "\n",
        "\n",
        "You can see how much memory you have available at any time by running the following code cell. If the execution result of running the code cell below is \"Not using a high-RAM runtime\", then you can enable a high-RAM runtime via `Runtime > Change runtime type` in the menu. Then select High-RAM in the Runtime shape dropdown. After, re-execute the code cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1G82GuO-tez",
        "outputId": "3438f334-fa20-442f-ec9e-4d28da8f5431"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 89.6 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJW8Qi-pPpep"
      },
      "source": [
        "## Longer runtimes\n",
        "\n",
        "All Colab runtimes are reset after some period of time (which is faster if the runtime isn't executing code). Colab Pro and Pro+ users have access to longer runtimes than those who use Colab free of charge.\n",
        "\n",
        "## Background execution\n",
        "\n",
        "Colab Pro+ users have access to background execution, where notebooks will continue executing even after you've closed a browser tab. This is always enabled in Pro+ runtimes as long as you have compute units available.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLlTRcMM_h0k"
      },
      "source": [
        "## Relaxing resource limits in Colab Pro\n",
        "\n",
        "Your resources are not unlimited in Colab. To make the most of Colab, avoid using resources when you don't need them. For example, only use a GPU when required and close Colab tabs when finished.\n",
        "\n",
        "\n",
        "\n",
        "If you encounter limitations, you can relax those limitations by purchasing more compute units via Pay As You Go. Anyone can purchase compute units via [Pay As You Go](https://colab.research.google.com/signup); no subscription is required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mm8FzEidvPs6"
      },
      "source": [
        "## Send us feedback!\n",
        "\n",
        "If you have any feedback for us, please let us know. The best way to send feedback is by using the Help > 'Send feedback...' menu. If you encounter usage limits in Colab Pro consider subscribing to Pro+.\n",
        "\n",
        "If you encounter errors or other issues with billing (payments) for Colab Pro, Pro+, or Pay As You Go, please email [colab-billing@google.com](mailto:colab-billing@google.com)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qB3bdLe8jkAa"
      },
      "source": [
        "## More Resources\n",
        "\n",
        "### Working with Notebooks in Colab\n",
        "- [Overview of Colaboratory](/notebooks/basic_features_overview.ipynb)\n",
        "- [Guide to Markdown](/notebooks/markdown_guide.ipynb)\n",
        "- [Importing libraries and installing dependencies](/notebooks/snippets/importing_libraries.ipynb)\n",
        "- [Saving and loading notebooks in GitHub](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb)\n",
        "- [Interactive forms](/notebooks/forms.ipynb)\n",
        "- [Interactive widgets](/notebooks/widgets.ipynb)\n",
        "\n",
        "<a name=\"working-with-data\"></a>\n",
        "### Working with Data\n",
        "- [Loading data: Drive, Sheets, and Google Cloud Storage](/notebooks/io.ipynb)\n",
        "- [Charts: visualizing data](/notebooks/charts.ipynb)\n",
        "- [Getting started with BigQuery](/notebooks/bigquery.ipynb)\n",
        "\n",
        "### Machine Learning Crash Course\n",
        "These are a few of the notebooks from Google's online Machine Learning course. See the [full course website](https://developers.google.com/machine-learning/crash-course/) for more.\n",
        "- [Intro to Pandas DataFrame](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/pandas_dataframe_ultraquick_tutorial.ipynb)\n",
        "- [Linear regression with tf.keras using synthetic data](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/linear_regression_with_synthetic_data.ipynb)\n",
        "\n",
        "\n",
        "<a name=\"using-accelerated-hardware\"></a>\n",
        "### Using Accelerated Hardware\n",
        "- [TensorFlow with GPUs](/notebooks/gpu.ipynb)\n",
        "- [TensorFlow with TPUs](/notebooks/tpu.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFm2S0Gijqo8"
      },
      "source": [
        "<a name=\"machine-learning-examples\"></a>\n",
        "\n",
        "## Machine Learning Examples\n",
        "\n",
        "To see end-to-end examples of the interactive machine learning analyses that Colaboratory makes possible, check out these  tutorials using models from [TensorFlow Hub](https://tfhub.dev).\n",
        "\n",
        "A few featured examples:\n",
        "\n",
        "- [Retraining an Image Classifier](https://tensorflow.org/hub/tutorials/tf2_image_retraining): Build a Keras model on top of a pre-trained image classifier to distinguish flowers.\n",
        "- [Text Classification](https://tensorflow.org/hub/tutorials/tf2_text_classification): Classify IMDB movie reviews as either *positive* or *negative*.\n",
        "- [Style Transfer](https://tensorflow.org/hub/tutorials/tf2_arbitrary_image_stylization): Use deep learning to transfer style between images.\n",
        "- [Multilingual Universal Sentence Encoder Q&A](https://tensorflow.org/hub/tutorials/retrieval_with_tf_hub_universal_encoder_qa): Use a machine learning model to answer questions from the SQuAD dataset.\n",
        "- [Video Interpolation](https://tensorflow.org/hub/tutorials/tweening_conv3d): Predict what happened in a video between the first and the last frame.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "VcEgNEqGCBeD",
        "outputId": "71ca31b2-c3e3-4c9d-db25-0dea18a3de9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone \"https://github.com/csebuetnlp/BanglaNLG\""
      ],
      "metadata": {
        "id": "vetkhFrG8jM5",
        "outputId": "bc86591b-fc5c-4003-9cc6-ef5338e369c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'BanglaNLG'...\n",
            "remote: Enumerating objects: 82, done.\u001b[K\n",
            "remote: Counting objects: 100% (82/82), done.\u001b[K\n",
            "remote: Compressing objects: 100% (57/57), done.\u001b[K\n",
            "remote: Total 82 (delta 39), reused 53 (delta 23), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (82/82), 1.29 MiB | 6.55 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! conda create python==3.7.9 pytorch==1.8.1 torchvision==0.9.1 torchaudio==0.8.0 cudatoolkit=10.2 -c pytorch -p ./env\n",
        "! conda activate ./env\n",
        "! bash \"/content/BanglaNLG/setup.sh\""
      ],
      "metadata": {
        "id": "2OWs7xk890vu",
        "outputId": "ab21318c-cd7e-4266-d176-a2bc99433974",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: conda: command not found\n",
            "/bin/bash: conda: command not found\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.8.1+cu111\n",
            "  Downloading https://download.pytorch.org/whl/cu111/torch-1.8.1%2Bcu111-cp38-cp38-linux_x86_64.whl (1982.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 GB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.9.1+cu111\n",
            "  Downloading https://download.pytorch.org/whl/cu111/torchvision-0.9.1%2Bcu111-cp38-cp38-linux_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==0.8.1\n",
            "  Downloading torchaudio-0.8.1-cp38-cp38-manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.8.1+cu111) (4.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torch==1.8.1+cu111) (1.21.6)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.8/dist-packages (from torchvision==0.9.1+cu111) (7.1.2)\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.1+cu116\n",
            "    Uninstalling torch-1.13.1+cu116:\n",
            "      Successfully uninstalled torch-1.13.1+cu116\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.14.1+cu116\n",
            "    Uninstalling torchvision-0.14.1+cu116:\n",
            "      Successfully uninstalled torchvision-0.14.1+cu116\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 0.13.1+cu116\n",
            "    Uninstalling torchaudio-0.13.1+cu116:\n",
            "      Successfully uninstalled torchaudio-0.13.1+cu116\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.14.1 requires torch==1.13.1, but you have torch 1.8.1+cu111 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-1.8.1+cu111 torchaudio-0.8.1 torchvision-0.9.1+cu111\n",
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 130592, done.\u001b[K\n",
            "remote: Counting objects: 100% (172/172), done.\u001b[K\n",
            "remote: Compressing objects: 100% (143/143), done.\u001b[K\n",
            "remote: Total 130592 (delta 66), reused 71 (delta 20), pack-reused 130420\u001b[K\n",
            "Receiving objects: 100% (130592/130592), 125.83 MiB | 16.19 MiB/s, done.\n",
            "Resolving deltas: 100% (98545/98545), done.\n",
            "Note: switching to '7a26307e3186926373cf9129248c209ab869148b'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by switching back to a branch.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -c with the switch command. Example:\n",
            "\n",
            "  git switch -c <new-branch-name>\n",
            "\n",
            "Or undo this operation with:\n",
            "\n",
            "  git switch -\n",
            "\n",
            "Turn off this advice by setting config variable advice.detachedHead to false\n",
            "\n",
            "HEAD is now at 7a26307e3 Fixes for the documentation (#13361)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing /content/transformers\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.11.0.dev0) (1.21.6)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 KB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from transformers==4.11.0.dev0) (23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==4.11.0.dev0) (3.9.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.11.0.dev0) (2022.6.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers==4.11.0.dev0) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers==4.11.0.dev0) (2.25.1)\n",
            "Collecting huggingface-hub>=0.0.12\n",
            "  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m89.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers==4.11.0.dev0) (4.64.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.0.12->transformers==4.11.0.dev0) (4.5.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.11.0.dev0) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.11.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.11.0.dev0) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.11.0.dev0) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==4.11.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==4.11.0.dev0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==4.11.0.dev0) (1.2.0)\n",
            "Building wheels for collected packages: transformers, sacremoses\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.11.0.dev0-py3-none-any.whl size=2811126 sha256=2c06db65c7d7248885ec34dfac023fa5c66a42082d697a3058fccecd7d7f21ee\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-rjuxqoz4/wheels/15/57/14/0d2873a0295966ca166ea9d9225761a50cce27e4d6b0341fcc\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=af87deaa816bf079f71bb5edfcf886cedb2f729c448003b9e7be1ae521812ee5\n",
            "  Stored in directory: /root/.cache/pip/wheels/82/ab/9b/c15899bf659ba74f623ac776e861cf2eb8608c1825ddec66a4\n",
            "Successfully built transformers sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.12.1 sacremoses-0.0.53 tokenizers-0.10.3 transformers-4.11.0.dev0\n",
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==1.8.1 torchvision==0.9.1"
      ],
      "metadata": {
        "id": "i8AqrUAx-1mF",
        "outputId": "d0fbb2da-e24b-4054-eed2-c10f2daf4794",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch==1.8.1 in /usr/local/lib/python3.8/dist-packages (1.8.1+cu111)\n",
            "Requirement already satisfied: torchvision==0.9.1 in /usr/local/lib/python3.8/dist-packages (0.9.1+cu111)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torch==1.8.1) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.8.1) (4.5.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.8/dist-packages (from torchvision==0.9.1) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade -r /content/BanglaNLG/requirements.txt"
      ],
      "metadata": {
        "id": "vZ2kzhFc_t2y",
        "outputId": "772e834f-d535-40f8-8326-5594ba74d6a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/csebuetnlp/normalizer (from -r /content/BanglaNLG/requirements.txt (line 6))\n",
            "  Cloning https://github.com/csebuetnlp/normalizer to /tmp/pip-req-build-r61uk921\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/csebuetnlp/normalizer /tmp/pip-req-build-r61uk921\n",
            "  Resolved https://github.com/csebuetnlp/normalizer to commit d80c3c484e1b80268f2b2dfaf7557fe65e34f321\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting git+https://github.com/otuncelli/turkish-stemmer-python (from -r /content/BanglaNLG/requirements.txt (line 7))\n",
            "  Cloning https://github.com/otuncelli/turkish-stemmer-python to /tmp/pip-req-build-jfjnmxwu\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/otuncelli/turkish-stemmer-python /tmp/pip-req-build-jfjnmxwu\n",
            "  Resolved https://github.com/otuncelli/turkish-stemmer-python to commit 0c22380bf84a5ab1f219f4a905274c78afa04ed1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting git+https://github.com/abhik1505040/bengali-stemmer (from -r /content/BanglaNLG/requirements.txt (line 8))\n",
            "  Cloning https://github.com/abhik1505040/bengali-stemmer to /tmp/pip-req-build-md8m4r80\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/abhik1505040/bengali-stemmer /tmp/pip-req-build-md8m4r80\n",
            "  Resolved https://github.com/abhik1505040/bengali-stemmer to commit 375186caee8e50e3260dd6bc02d20d50277f3e39\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting git+https://github.com/abhik1505040/indic_nlp_library (from -r /content/BanglaNLG/requirements.txt (line 9))\n",
            "  Cloning https://github.com/abhik1505040/indic_nlp_library to /tmp/pip-req-build-aa7mnzjq\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/abhik1505040/indic_nlp_library /tmp/pip-req-build-aa7mnzjq\n",
            "  Resolved https://github.com/abhik1505040/indic_nlp_library to commit 201759042cbdee78d1208011e891ab300b4d6f07\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting rouge_score\n",
            "  Cloning https://github.com/csebuetnlp/xl-sum.git to /tmp/pip-install-9wy299ab/rouge-score_3da7fc7afe184c6da077b993db055d57\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/csebuetnlp/xl-sum.git /tmp/pip-install-9wy299ab/rouge-score_3da7fc7afe184c6da077b993db055d57\n",
            "  Resolved https://github.com/csebuetnlp/xl-sum.git to commit 6b6da9eb052060724e9ffb17d18c291e8c61b553\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sentencepiece!=0.1.92\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.8/dist-packages (from -r /content/BanglaNLG/requirements.txt (line 2)) (3.19.6)\n",
            "Collecting protobuf\n",
            "  Downloading protobuf-4.22.0-cp37-abi3-manylinux2014_x86_64.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.4/302.4 KB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets==1.11.0\n",
            "  Downloading datasets-1.11.0-py3-none-any.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.9/264.9 KB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting seqeval==1.2.2\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 KB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sacrebleu==1.4.2\n",
            "  Downloading sacrebleu-1.4.2-py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (from -r /content/BanglaNLG/requirements.txt (line 10)) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from -r /content/BanglaNLG/requirements.txt (line 11)) (3.7)\n",
            "Collecting nltk\n",
            "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m82.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from -r /content/BanglaNLG/requirements.txt (line 12)) (1.21.6)\n",
            "Collecting numpy\n",
            "  Downloading numpy-1.24.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.14 in /usr/local/lib/python3.8/dist-packages (from -r /content/BanglaNLG/requirements.txt (line 13)) (1.15.0)\n",
            "Collecting six>=1.14\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting pythainlp\n",
            "  Downloading pythainlp-3.1.1-py3-none-any.whl (9.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyonmttok\n",
            "  Downloading pyonmttok-1.36.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.7/16.7 MB\u001b[0m \u001b[31m85.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jieba in /usr/local/lib/python3.8/dist-packages (from -r /content/BanglaNLG/requirements.txt (line 16)) (0.42.1)\n",
            "Collecting fugashi[unidic]\n",
            "  Downloading fugashi-1.2.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.9/615.9 KB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<0.1.0\n",
            "  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 KB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets==1.11.0->-r /content/BanglaNLG/requirements.txt (line 3)) (2.25.1)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 KB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.42 in /usr/local/lib/python3.8/dist-packages (from datasets==1.11.0->-r /content/BanglaNLG/requirements.txt (line 3)) (4.64.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets==1.11.0->-r /content/BanglaNLG/requirements.txt (line 3)) (1.3.5)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.8/dist-packages (from datasets==1.11.0->-r /content/BanglaNLG/requirements.txt (line 3)) (2023.1.0)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets==1.11.0->-r /content/BanglaNLG/requirements.txt (line 3)) (9.0.0)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 KB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.8/dist-packages (from datasets==1.11.0->-r /content/BanglaNLG/requirements.txt (line 3)) (0.3.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets==1.11.0->-r /content/BanglaNLG/requirements.txt (line 3)) (23.0)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.8/dist-packages (from seqeval==1.2.2->-r /content/BanglaNLG/requirements.txt (line 4)) (1.0.2)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting typing\n",
            "  Downloading typing-3.7.4.3.tar.gz (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 KB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from normalizer==0.0.1->-r /content/BanglaNLG/requirements.txt (line 6)) (2022.6.2)\n",
            "Collecting emoji==1.4.2\n",
            "  Downloading emoji-1.4.2.tar.gz (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m185.0/185.0 KB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy==6.0.3\n",
            "  Downloading ftfy-6.0.3.tar.gz (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 KB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from ftfy==6.0.3->normalizer==0.0.1->-r /content/BanglaNLG/requirements.txt (line 6)) (0.2.6)\n",
            "Collecting sphinx-argparse\n",
            "  Downloading sphinx_argparse-0.4.0-py3-none-any.whl (12 kB)\n",
            "Collecting sphinx_rtd_theme\n",
            "  Downloading sphinx_rtd_theme-1.2.0-py2.py3-none-any.whl (2.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m99.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting morfessor\n",
            "  Downloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\n",
            "Collecting GitPython\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk->-r /content/BanglaNLG/requirements.txt (line 11)) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk->-r /content/BanglaNLG/requirements.txt (line 11)) (7.1.2)\n",
            "Collecting unidic\n",
            "  Downloading unidic-1.1.0.tar.gz (7.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<0.1.0->datasets==1.11.0->-r /content/BanglaNLG/requirements.txt (line 3)) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<0.1.0->datasets==1.11.0->-r /content/BanglaNLG/requirements.txt (line 3)) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<0.1.0->datasets==1.11.0->-r /content/BanglaNLG/requirements.txt (line 3)) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets==1.11.0->-r /content/BanglaNLG/requirements.txt (line 3)) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets==1.11.0->-r /content/BanglaNLG/requirements.txt (line 3)) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets==1.11.0->-r /content/BanglaNLG/requirements.txt (line 3)) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets==1.11.0->-r /content/BanglaNLG/requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.3->seqeval==1.2.2->-r /content/BanglaNLG/requirements.txt (line 4)) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.3->seqeval==1.2.2->-r /content/BanglaNLG/requirements.txt (line 4)) (3.1.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets==1.11.0->-r /content/BanglaNLG/requirements.txt (line 3)) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets==1.11.0->-r /content/BanglaNLG/requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: sphinx>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from sphinx-argparse->indic-nlp-library==0.81->-r /content/BanglaNLG/requirements.txt (line 9)) (3.5.4)\n",
            "Requirement already satisfied: docutils<0.19 in /usr/local/lib/python3.8/dist-packages (from sphinx_rtd_theme->indic-nlp-library==0.81->-r /content/BanglaNLG/requirements.txt (line 9)) (0.16)\n",
            "Collecting sphinxcontrib-jquery!=3.0.0,>=2.0.0\n",
            "  Downloading sphinxcontrib_jquery-2.0.0-py3-none-any.whl (3.2 kB)\n",
            "Requirement already satisfied: wasabi<1.0.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from unidic->fugashi[unidic]->-r /content/BanglaNLG/requirements.txt (line 17)) (0.10.1)\n",
            "Collecting plac<2.0.0,>=1.1.3\n",
            "  Downloading plac-1.3.5-py2.py3-none-any.whl (22 kB)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Collecting numpy\n",
            "  Downloading numpy-1.22.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.8/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library==0.81->-r /content/BanglaNLG/requirements.txt (line 9)) (1.0.2)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.8/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library==0.81->-r /content/BanglaNLG/requirements.txt (line 9)) (1.0.1)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.8/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library==0.81->-r /content/BanglaNLG/requirements.txt (line 9)) (1.4.1)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.8/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library==0.81->-r /content/BanglaNLG/requirements.txt (line 9)) (2.2.0)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.8/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library==0.81->-r /content/BanglaNLG/requirements.txt (line 9)) (2.11.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library==0.81->-r /content/BanglaNLG/requirements.txt (line 9)) (57.4.0)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp in /usr/local/lib/python3.8/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library==0.81->-r /content/BanglaNLG/requirements.txt (line 9)) (2.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.8/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library==0.81->-r /content/BanglaNLG/requirements.txt (line 9)) (1.0.4)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.8/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library==0.81->-r /content/BanglaNLG/requirements.txt (line 9)) (2.6.1)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.8/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library==0.81->-r /content/BanglaNLG/requirements.txt (line 9)) (0.7.13)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.8/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library==0.81->-r /content/BanglaNLG/requirements.txt (line 9)) (1.0.3)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.8/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library==0.81->-r /content/BanglaNLG/requirements.txt (line 9)) (2.11.3)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.8/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library==0.81->-r /content/BanglaNLG/requirements.txt (line 9)) (1.1.5)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from Jinja2>=2.3->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library==0.81->-r /content/BanglaNLG/requirements.txt (line 9)) (2.0.1)\n",
            "Building wheels for collected packages: seqeval, normalizer, emoji, ftfy, TurkishStemmer, bengali-stemmer, indic-nlp-library, rouge_score, typing, unidic\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16179 sha256=916c9ccf6c0e42cbe9997ef04fb98739ac230600e5a44b1fea70117a4c93822f\n",
            "  Stored in directory: /root/.cache/pip/wheels/ad/5c/ba/05fa33fa5855777b7d686e843ec07452f22a66a138e290e732\n",
            "  Building wheel for normalizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for normalizer: filename=normalizer-0.0.1-py3-none-any.whl size=6883 sha256=e9dcb910ec96e0c353a6a4083c5dd567a96b6b5cacae600e282d568ae15290cf\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-bhque1du/wheels/8e/25/ac/a3666919774bd6e5a7818bc8630b4cfce6b3900f6299a261b6\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.4.2-py3-none-any.whl size=186469 sha256=9291a56a56fc17d2562a7b81821d4d7d1bbbb8f4117b4e76651d67335d8f38f7\n",
            "  Stored in directory: /root/.cache/pip/wheels/71/4d/3c/cada364d4ea0026deee7208dee1e61bcebd20aa2ae5dc154ba\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-6.0.3-py3-none-any.whl size=41933 sha256=7e692f11fb1b985e04cdc7f95f9dcb65a904210d4ff4d6c930d75183ffac53a7\n",
            "  Stored in directory: /root/.cache/pip/wheels/7f/40/63/4bf603cec3ecc4a26985405834cb47eb8368bfa59e15dde046\n",
            "  Building wheel for TurkishStemmer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for TurkishStemmer: filename=TurkishStemmer-1.3-py3-none-any.whl size=19871 sha256=800ca8189b518baea25555afbc5533756a655423b8a15862778df4b448ae671d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-bhque1du/wheels/25/92/5f/004e4b96ab2c0202e1c3c2c19697f43397af070e53861714cb\n",
            "  Building wheel for bengali-stemmer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bengali-stemmer: filename=bengali_stemmer-0.0.1-py2.py3-none-any.whl size=6408 sha256=5d1794c88497a4aa86b98e42257e11d430bbde8ba1a972ee9c015ca13c95520d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-bhque1du/wheels/75/cb/a8/ab10fc67dd2eda339a66caaabc2a1456b2c50bcbe3ae61ffd8\n",
            "  Building wheel for indic-nlp-library (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for indic-nlp-library: filename=indic_nlp_library-0.81-py3-none-any.whl size=40399 sha256=8ab575149fc58dcdead41d29d8408fa622519552f206b3718426b1e9f786c4a8\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-bhque1du/wheels/49/d9/fe/987f9ef57974ac2122ff2cf460c5fd8738d07f1bb15d72fd37\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.0.0-py3-none-any.whl size=18181 sha256=26dafdbbcb2ec1a14dcfe8849839530a1f3006691dcd6828b0079813958b68b1\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-bhque1du/wheels/45/d5/68/0f3ebdaceab43892a7f07f019025f74a244dd3a7186cafd57d\n",
            "  Building wheel for typing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for typing: filename=typing-3.7.4.3-py3-none-any.whl size=26323 sha256=ffee4bb1c5539ca7b6ad97c440e3f3afb570c50b5bc7a9c099913fb30cb29f93\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/5d/01/3083e091b57809dad979ea543def62d9d878950e3e74f0c930\n",
            "  Building wheel for unidic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unidic: filename=unidic-1.1.0-py3-none-any.whl size=7425 sha256=a5095b760cb0f896a87185776068b868bf803438410c7e6a5f5137872e2d2676\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/bc/bb/46aba36d0388f67dfe44bb0edc20a2c964560d4d19ec394e05\n",
            "Successfully built seqeval normalizer emoji ftfy TurkishStemmer bengali-stemmer indic-nlp-library rouge_score typing unidic\n",
            "Installing collected packages: TurkishStemmer, sentencepiece, plac, morfessor, emoji, bengali-stemmer, xxhash, typing, sphinxcontrib-jquery, smmap, six, pyonmttok, protobuf, portalocker, numpy, nltk, multiprocess, fugashi, ftfy, unidic, sacrebleu, rouge_score, pythainlp, normalizer, huggingface-hub, gitdb, sphinx_rtd_theme, sphinx-argparse, GitPython, seqeval, indic-nlp-library, datasets\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.15.0\n",
            "    Uninstalling six-1.15.0:\n",
            "      Successfully uninstalled six-1.15.0\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.19.6\n",
            "    Uninstalling protobuf-3.19.6:\n",
            "      Successfully uninstalled protobuf-3.19.6\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.7\n",
            "    Uninstalling nltk-3.7:\n",
            "      Successfully uninstalled nltk-3.7\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.12.1\n",
            "    Uninstalling huggingface-hub-0.12.1:\n",
            "      Successfully uninstalled huggingface-hub-0.12.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.9.0 requires jedi>=0.10, which is not installed.\n",
            "torchtext 0.14.1 requires torch==1.13.1, but you have torch 1.8.1+cu111 which is incompatible.\n",
            "tensorflow 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.22.0 which is incompatible.\n",
            "tensorflow-metadata 1.12.0 requires protobuf<4,>=3.13, but you have protobuf 4.22.0 which is incompatible.\n",
            "tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.22.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed GitPython-3.1.31 TurkishStemmer-1.3 bengali-stemmer-0.0.1 datasets-1.11.0 emoji-1.4.2 ftfy-6.0.3 fugashi-1.2.1 gitdb-4.0.10 huggingface-hub-0.0.19 indic-nlp-library-0.81 morfessor-2.0.6 multiprocess-0.70.14 nltk-3.8.1 normalizer-0.0.1 numpy-1.22.4 plac-1.3.5 portalocker-2.7.0 protobuf-4.22.0 pyonmttok-1.36.0 pythainlp-3.1.1 rouge_score-0.0.0 sacrebleu-1.4.2 sentencepiece-0.1.97 seqeval-1.2.2 six-1.16.0 smmap-5.0.0 sphinx-argparse-0.4.0 sphinx_rtd_theme-1.2.0 sphinxcontrib-jquery-2.0.0 typing-3.7.4.3 unidic-1.1.0 xxhash-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install protobuf==3.20"
      ],
      "metadata": {
        "id": "21yeM3mKACwe",
        "outputId": "20cd60de-5de8-422f-925e-d7a3629e420e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting protobuf==3.20\n",
            "  Downloading protobuf-3.20.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.22.0\n",
            "    Uninstalling protobuf-4.22.0:\n",
            "      Successfully uninstalled protobuf-4.22.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.0 which is incompatible.\n",
            "googleapis-common-protos 1.58.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-translate 3.8.4 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-language 2.6.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-firestore 2.7.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-datastore 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-bigquery 3.4.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.18.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-3.20.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!  python /content/BanglaNLG/seq2seq/run_seq2seq.py \\\n",
        "    --model_name_or_path \"csebuetnlp/banglat5\" \\\n",
        "    --dataset_dir \"/content/drive/MyDrive/ML project/Data/small\" \\\n",
        "    --output_dir \"/content/drive/MyDrive/ML project/Model/output_1\" \\\n",
        "    --learning_rate=5e-4 \\\n",
        "    --warmup_steps 5000 \\\n",
        "    --label_smoothing_factor 0.1 \\\n",
        "    --gradient_accumulation_steps 4 \\\n",
        "    --weight_decay 0.1 \\\n",
        "    --lr_scheduler_type \"linear\"  \\\n",
        "    --per_device_train_batch_size=8 \\\n",
        "    --per_device_eval_batch_size=8 \\\n",
        "    --max_source_length 256 \\\n",
        "    --max_target_length 256 \\\n",
        "    --logging_strategy \"epoch\" \\\n",
        "    --save_strategy \"epoch\" \\\n",
        "    --evaluation_strategy \"epoch\" \\\n",
        "    --source_key bn --target_key en \\\n",
        "    --greater_is_better true --load_best_model_at_end \\\n",
        "    --metric_for_best_model sacrebleu --evaluation_metric sacrebleu \\\n",
        "    --num_train_epochs 10 \\\n",
        "    --do_train --do_eval --do_predict \\\n",
        "    --predict_with_generate"
      ],
      "metadata": {
        "id": "YUqS4NIqBSJN",
        "outputId": "7955810f-756e-4637-d97a-b870b8cfa746",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-20 18:00:34.075786: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-20 18:00:34.224401: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-02-20 18:00:35.266289: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-20 18:00:35.266403: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-20 18:00:35.266424: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.EPOCH,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=4,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.1,\n",
            "learning_rate=0.0005,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/drive/MyDrive/ML project/Model/output_1/runs/Feb20_18-00-36_fe0de8f16b43,\n",
            "logging_first_step=False,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.EPOCH,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=sacrebleu,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=10.0,\n",
            "output_dir=/content/drive/MyDrive/ML project/Model/output_1,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "predict_with_generate=True,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=output_1,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=None,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/drive/MyDrive/ML project/Model/output_1,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.EPOCH,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=5000,\n",
            "weight_decay=0.1,\n",
            ")\n",
            "WARNING:datasets.builder:Using custom data configuration default-5e33587fe9de9890\n",
            "INFO:datasets.builder:Generating dataset json (/root/.cache/huggingface/datasets/json/default-5e33587fe9de9890/0.0.0)\n",
            "Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/json/default-5e33587fe9de9890/0.0.0...\n",
            "100% 3/3 [00:00<00:00, 4381.24it/s]\n",
            "INFO:datasets.utils.download_manager:Downloading took 0.0 min\n",
            "INFO:datasets.utils.download_manager:Checksum Computation took 0.0 min\n",
            "100% 3/3 [00:00<00:00, 120.47it/s]\n",
            "INFO:datasets.builder:Generating split train\n",
            "INFO:datasets.builder:Generating split validation\n",
            "INFO:datasets.builder:Generating split test\n",
            "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-5e33587fe9de9890/0.0.0. Subsequent calls will reuse this data.\n",
            "100% 3/3 [00:00<00:00, 68.01it/s]\n",
            "[INFO|file_utils.py:1665] 2023-02-20 18:00:40,887 >> https://huggingface.co/csebuetnlp/banglat5/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpwl2s6nzw\n",
            "Downloading: 100% 659/659 [00:00<00:00, 619kB/s]\n",
            "[INFO|file_utils.py:1669] 2023-02-20 18:00:41,790 >> storing https://huggingface.co/csebuetnlp/banglat5/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/63f644ae00c3dd6972aa91ba253a7351c7ba16c8c9b3696aaff46a4a12684633.300a3d751aa65d7afe96f1b6315604efbd372f3e517a18789d224498d5c2cc19\n",
            "[INFO|file_utils.py:1677] 2023-02-20 18:00:41,790 >> creating metadata file for /root/.cache/huggingface/transformers/63f644ae00c3dd6972aa91ba253a7351c7ba16c8c9b3696aaff46a4a12684633.300a3d751aa65d7afe96f1b6315604efbd372f3e517a18789d224498d5c2cc19\n",
            "[INFO|configuration_utils.py:561] 2023-02-20 18:00:41,791 >> loading configuration file https://huggingface.co/csebuetnlp/banglat5/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/63f644ae00c3dd6972aa91ba253a7351c7ba16c8c9b3696aaff46a4a12684633.300a3d751aa65d7afe96f1b6315604efbd372f3e517a18789d224498d5c2cc19\n",
            "[INFO|configuration_utils.py:598] 2023-02-20 18:00:41,792 >> Model config T5Config {\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1665] 2023-02-20 18:00:42,699 >> https://huggingface.co/csebuetnlp/banglat5/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpgd59c5is\n",
            "Downloading: 100% 1.83k/1.83k [00:00<00:00, 1.80MB/s]\n",
            "[INFO|file_utils.py:1669] 2023-02-20 18:00:43,604 >> storing https://huggingface.co/csebuetnlp/banglat5/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/abd4e91d8505fbba650381ad95a0d1ac188754eba6e9a336369e573223be0a32.98e6d2ca7aa4fc72eb77de3e453d11925dfe2d3289ad7507713b298dc17b4109\n",
            "[INFO|file_utils.py:1677] 2023-02-20 18:00:43,604 >> creating metadata file for /root/.cache/huggingface/transformers/abd4e91d8505fbba650381ad95a0d1ac188754eba6e9a336369e573223be0a32.98e6d2ca7aa4fc72eb77de3e453d11925dfe2d3289ad7507713b298dc17b4109\n",
            "[INFO|configuration_utils.py:561] 2023-02-20 18:00:44,507 >> loading configuration file https://huggingface.co/csebuetnlp/banglat5/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/63f644ae00c3dd6972aa91ba253a7351c7ba16c8c9b3696aaff46a4a12684633.300a3d751aa65d7afe96f1b6315604efbd372f3e517a18789d224498d5c2cc19\n",
            "[INFO|configuration_utils.py:598] 2023-02-20 18:00:44,507 >> Model config T5Config {\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1665] 2023-02-20 18:00:46,331 >> https://huggingface.co/csebuetnlp/banglat5/resolve/main/spiece.model not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpp3pi9l3u\n",
            "Downloading: 100% 1.11M/1.11M [00:01<00:00, 1.05MB/s]\n",
            "[INFO|file_utils.py:1669] 2023-02-20 18:00:48,380 >> storing https://huggingface.co/csebuetnlp/banglat5/resolve/main/spiece.model in cache at /root/.cache/huggingface/transformers/2bdec5bf71b4e9bf56f02cb3529bf11581596c07acb1fa384cef5bcbd2c54bff.33c52581428923e4e81635e99dd0211109e727e4068e96fb9a1863b848bbd0b7\n",
            "[INFO|file_utils.py:1677] 2023-02-20 18:00:48,380 >> creating metadata file for /root/.cache/huggingface/transformers/2bdec5bf71b4e9bf56f02cb3529bf11581596c07acb1fa384cef5bcbd2c54bff.33c52581428923e4e81635e99dd0211109e727e4068e96fb9a1863b848bbd0b7\n",
            "[INFO|file_utils.py:1665] 2023-02-20 18:00:50,202 >> https://huggingface.co/csebuetnlp/banglat5/resolve/main/special_tokens_map.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp4_5p8by3\n",
            "Downloading: 100% 1.79k/1.79k [00:00<00:00, 1.75MB/s]\n",
            "[INFO|file_utils.py:1669] 2023-02-20 18:00:51,111 >> storing https://huggingface.co/csebuetnlp/banglat5/resolve/main/special_tokens_map.json in cache at /root/.cache/huggingface/transformers/ff0ed476d41a6f336fa52bd906c6c8f0a8684fe67bec634b201ed2d24331c915.c94798918c92ded6aeef2d2f0e666d2cc4145eca1aa6e1336fde07f2e13e2f46\n",
            "[INFO|file_utils.py:1677] 2023-02-20 18:00:51,111 >> creating metadata file for /root/.cache/huggingface/transformers/ff0ed476d41a6f336fa52bd906c6c8f0a8684fe67bec634b201ed2d24331c915.c94798918c92ded6aeef2d2f0e666d2cc4145eca1aa6e1336fde07f2e13e2f46\n",
            "[INFO|tokenization_utils_base.py:1739] 2023-02-20 18:00:52,939 >> loading file https://huggingface.co/csebuetnlp/banglat5/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/2bdec5bf71b4e9bf56f02cb3529bf11581596c07acb1fa384cef5bcbd2c54bff.33c52581428923e4e81635e99dd0211109e727e4068e96fb9a1863b848bbd0b7\n",
            "[INFO|tokenization_utils_base.py:1739] 2023-02-20 18:00:52,939 >> loading file https://huggingface.co/csebuetnlp/banglat5/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1739] 2023-02-20 18:00:52,939 >> loading file https://huggingface.co/csebuetnlp/banglat5/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/ff0ed476d41a6f336fa52bd906c6c8f0a8684fe67bec634b201ed2d24331c915.c94798918c92ded6aeef2d2f0e666d2cc4145eca1aa6e1336fde07f2e13e2f46\n",
            "[INFO|tokenization_utils_base.py:1739] 2023-02-20 18:00:52,939 >> loading file https://huggingface.co/csebuetnlp/banglat5/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/abd4e91d8505fbba650381ad95a0d1ac188754eba6e9a336369e573223be0a32.98e6d2ca7aa4fc72eb77de3e453d11925dfe2d3289ad7507713b298dc17b4109\n",
            "[INFO|tokenization_utils_base.py:1739] 2023-02-20 18:00:52,939 >> loading file https://huggingface.co/csebuetnlp/banglat5/resolve/main/tokenizer.json from cache at None\n",
            "[INFO|configuration_utils.py:561] 2023-02-20 18:00:53,849 >> loading configuration file https://huggingface.co/csebuetnlp/banglat5/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/63f644ae00c3dd6972aa91ba253a7351c7ba16c8c9b3696aaff46a4a12684633.300a3d751aa65d7afe96f1b6315604efbd372f3e517a18789d224498d5c2cc19\n",
            "[INFO|configuration_utils.py:598] 2023-02-20 18:00:53,849 >> Model config T5Config {\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1665] 2023-02-20 18:00:54,846 >> https://huggingface.co/csebuetnlp/banglat5/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpxtpaojs4\n",
            "Downloading: 100% 990M/990M [01:06<00:00, 14.9MB/s]\n",
            "[INFO|file_utils.py:1669] 2023-02-20 18:02:02,549 >> storing https://huggingface.co/csebuetnlp/banglat5/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/5fe94a1245ced65384854b52d05810094e6a9dab5f53304383fc198c94e36c6b.65c7086f8ce77cdf063b7b388766fa16ba435cffcb09eb8fde858fa0fb94513e\n",
            "[INFO|file_utils.py:1677] 2023-02-20 18:02:02,549 >> creating metadata file for /root/.cache/huggingface/transformers/5fe94a1245ced65384854b52d05810094e6a9dab5f53304383fc198c94e36c6b.65c7086f8ce77cdf063b7b388766fa16ba435cffcb09eb8fde858fa0fb94513e\n",
            "[INFO|modeling_utils.py:1279] 2023-02-20 18:02:02,549 >> loading weights file https://huggingface.co/csebuetnlp/banglat5/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/5fe94a1245ced65384854b52d05810094e6a9dab5f53304383fc198c94e36c6b.65c7086f8ce77cdf063b7b388766fa16ba435cffcb09eb8fde858fa0fb94513e\n",
            "[INFO|modeling_utils.py:1524] 2023-02-20 18:02:06,019 >> All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
            "\n",
            "[INFO|modeling_utils.py:1532] 2023-02-20 18:02:06,019 >> All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at csebuetnlp/banglat5.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
            "WARNING:datasets.fingerprint:Parameter 'function'=<function main.<locals>.preprocess_function at 0x7fe024a8d550> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "Running tokenizer on train dataset:   0% 0/1939 [00:00<?, ?ba/s]/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:705: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  tensor = as_tensor(value)\n",
            "INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/json/default-5e33587fe9de9890/0.0.0/cache-1c80317fa3b1799d.arrow\n",
            "Running tokenizer on train dataset: 100% 1939/1939 [00:11<00:00, 166.88ba/s]\n",
            "INFO:datasets.fingerprint:Parameter 'function'=<function main.<locals>.preprocess_function at 0x7fe024a8d550> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n",
            "Running tokenizer on validation dataset:   0% 0/416 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/json/default-5e33587fe9de9890/0.0.0/cache-bdd640fb06671ad1.arrow\n",
            "Running tokenizer on validation dataset: 100% 416/416 [00:02<00:00, 166.78ba/s]\n",
            "INFO:datasets.fingerprint:Parameter 'function'=<function main.<locals>.preprocess_function at 0x7fe024a8d550> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n",
            "Running tokenizer on prediction dataset:   0% 0/416 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/json/default-5e33587fe9de9890/0.0.0/cache-3eb13b9046685257.arrow\n",
            "Running tokenizer on prediction dataset: 100% 416/416 [00:02<00:00, 165.66ba/s]\n",
            "[INFO|trainer.py:1168] 2023-02-20 18:02:28,880 >> ***** Running training *****\n",
            "[INFO|trainer.py:1169] 2023-02-20 18:02:28,880 >>   Num examples = 15509\n",
            "[INFO|trainer.py:1170] 2023-02-20 18:02:28,880 >>   Num Epochs = 10\n",
            "[INFO|trainer.py:1171] 2023-02-20 18:02:28,880 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1172] 2023-02-20 18:02:28,880 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:1173] 2023-02-20 18:02:28,880 >>   Gradient Accumulation steps = 4\n",
            "[INFO|trainer.py:1174] 2023-02-20 18:02:28,880 >>   Total optimization steps = 4840\n",
            "  5% 232/4840 [02:02<40:19,  1.90it/s]"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuClass": "premium"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}