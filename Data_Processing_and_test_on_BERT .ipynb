{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up banglabert repository"
      ],
      "metadata": {
        "id": "Hl2osI0DIRV0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewopmT02iFz3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "403913f5-0148-4a62-9ef3-82ef558fe4d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'banglabert'...\n",
            "remote: Enumerating objects: 141, done.\u001b[K\n",
            "remote: Counting objects: 100% (141/141), done.\u001b[K\n",
            "remote: Compressing objects: 100% (116/116), done.\u001b[K\n",
            "remote: Total 141 (delta 70), reused 73 (delta 23), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (141/141), 1.11 MiB | 2.60 MiB/s, done.\n",
            "Resolving deltas: 100% (70/70), done.\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/csebuetnlp/banglabert.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ah9M52thmzbm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d52b279-02dd-4869-8024-9c24183b87ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: conda: command not found\n",
            "/bin/bash: conda: command not found\n",
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 133140, done.\u001b[K\n",
            "remote: Counting objects: 100% (27/27), done.\u001b[K\n",
            "remote: Compressing objects: 100% (22/22), done.\u001b[K\n",
            "remote: Total 133140 (delta 9), reused 7 (delta 1), pack-reused 133113\u001b[K\n",
            "Receiving objects: 100% (133140/133140), 127.30 MiB | 18.38 MiB/s, done.\n",
            "Resolving deltas: 100% (100794/100794), done.\n",
            "Note: switching to '7a26307e3186926373cf9129248c209ab869148b'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by switching back to a branch.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -c with the switch command. Example:\n",
            "\n",
            "  git switch -c <new-branch-name>\n",
            "\n",
            "Or undo this operation with:\n",
            "\n",
            "  git switch -\n",
            "\n",
            "Turn off this advice by setting config variable advice.detachedHead to false\n",
            "\n",
            "HEAD is now at 7a26307e3 Fixes for the documentation (#13361)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing /content/transformers\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 KB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.0.12\n",
            "  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers==4.11.0.dev0) (2.25.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from transformers==4.11.0.dev0) (23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==4.11.0.dev0) (3.9.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.11.0.dev0) (2022.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers==4.11.0.dev0) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.11.0.dev0) (1.22.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers==4.11.0.dev0) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.0.12->transformers==4.11.0.dev0) (4.5.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.11.0.dev0) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.11.0.dev0) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.11.0.dev0) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.11.0.dev0) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==4.11.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==4.11.0.dev0) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==4.11.0.dev0) (1.2.0)\n",
            "Building wheels for collected packages: transformers, sacremoses\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.11.0.dev0-py3-none-any.whl size=2811126 sha256=716588887dda4c569e0af705d90a592006a201c0905473cb2a224f20ef3742ad\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-zbdbbon_/wheels/15/57/14/0d2873a0295966ca166ea9d9225761a50cce27e4d6b0341fcc\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=e21b47ea77bfe31a4643d097b2ba71a7c5a5b334356ae41306256b17432ff99e\n",
            "  Stored in directory: /root/.cache/pip/wheels/82/ab/9b/c15899bf659ba74f623ac776e861cf2eb8608c1825ddec66a4\n",
            "Successfully built transformers sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.12.1 sacremoses-0.0.53 tokenizers-0.10.3 transformers-4.11.0.dev0\n",
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "! conda create python==3.7.9 pytorch==1.8.1 torchvision==0.9.1 torchaudio==0.8.0 cudatoolkit=10.2 -c pytorch -p /content/banglabert/env\n",
        "! conda activate ./env # or source activate ./env (for older versions of anaconda)\n",
        "! bash /content/banglabert/setup.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEpEiy6foDx3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5fa0c4b-cd8e-4fd2-aeb0-f3fdab6627ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch==1.8.1\n",
            "  Downloading torch-1.8.1-cp38-cp38-manylinux1_x86_64.whl (804.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m804.1/804.1 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.9.1\n",
            "  Downloading torchvision-0.9.1-cp38-cp38-manylinux1_x86_64.whl (17.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.8.1) (4.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torch==1.8.1) (1.22.4)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.8/dist-packages (from torchvision==0.9.1) (8.4.0)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.1+cu116\n",
            "    Uninstalling torch-1.13.1+cu116:\n",
            "      Successfully uninstalled torch-1.13.1+cu116\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.14.1+cu116\n",
            "    Uninstalling torchvision-0.14.1+cu116:\n",
            "      Successfully uninstalled torchvision-0.14.1+cu116\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.14.1 requires torch==1.13.1, but you have torch 1.8.1 which is incompatible.\n",
            "torchaudio 0.13.1+cu116 requires torch==1.13.1, but you have torch 1.8.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-1.8.1 torchvision-0.9.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==1.8.1 torchvision==0.9.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYdt5oS8rm3s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fc5bcbb-935e-4d28-d6c5-07ffe577134f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/csebuetnlp/normalizer (from -r /content/banglabert/requirements.txt (line 5))\n",
            "  Cloning https://github.com/csebuetnlp/normalizer to /tmp/pip-req-build-j7xpxaxm\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/csebuetnlp/normalizer /tmp/pip-req-build-j7xpxaxm\n",
            "  Resolved https://github.com/csebuetnlp/normalizer to commit d80c3c484e1b80268f2b2dfaf7557fe65e34f321\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sentencepiece!=0.1.92\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.8/dist-packages (from -r /content/banglabert/requirements.txt (line 2)) (3.19.6)\n",
            "Collecting protobuf\n",
            "  Downloading protobuf-4.22.0-cp37-abi3-manylinux2014_x86_64.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.4/302.4 KB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets==1.11.0\n",
            "  Downloading datasets-1.11.0-py3-none-any.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.9/264.9 KB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting seqeval==1.2.2\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 KB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 KB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.42 in /usr/local/lib/python3.8/dist-packages (from datasets==1.11.0->-r /content/banglabert/requirements.txt (line 3)) (4.64.1)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets==1.11.0->-r /content/banglabert/requirements.txt (line 3)) (9.0.0)\n",
            "Collecting huggingface-hub<0.1.0\n",
            "  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 KB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets==1.11.0->-r /content/banglabert/requirements.txt (line 3)) (23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets==1.11.0->-r /content/banglabert/requirements.txt (line 3)) (1.22.4)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 KB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets==1.11.0->-r /content/banglabert/requirements.txt (line 3)) (2.25.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets==1.11.0->-r /content/banglabert/requirements.txt (line 3)) (1.3.5)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.8/dist-packages (from datasets==1.11.0->-r /content/banglabert/requirements.txt (line 3)) (2023.1.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.8/dist-packages (from datasets==1.11.0->-r /content/banglabert/requirements.txt (line 3)) (0.3.6)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.8/dist-packages (from seqeval==1.2.2->-r /content/banglabert/requirements.txt (line 4)) (1.0.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from normalizer==0.0.1->-r /content/banglabert/requirements.txt (line 5)) (2022.6.2)\n",
            "Collecting emoji==1.4.2\n",
            "  Downloading emoji-1.4.2.tar.gz (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m185.0/185.0 KB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy==6.0.3\n",
            "  Downloading ftfy-6.0.3.tar.gz (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 KB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from ftfy==6.0.3->normalizer==0.0.1->-r /content/banglabert/requirements.txt (line 5)) (0.2.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<0.1.0->datasets==1.11.0->-r /content/banglabert/requirements.txt (line 3)) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<0.1.0->datasets==1.11.0->-r /content/banglabert/requirements.txt (line 3)) (4.5.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<0.1.0->datasets==1.11.0->-r /content/banglabert/requirements.txt (line 3)) (6.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets==1.11.0->-r /content/banglabert/requirements.txt (line 3)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets==1.11.0->-r /content/banglabert/requirements.txt (line 3)) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets==1.11.0->-r /content/banglabert/requirements.txt (line 3)) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets==1.11.0->-r /content/banglabert/requirements.txt (line 3)) (1.26.14)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.3->seqeval==1.2.2->-r /content/banglabert/requirements.txt (line 4)) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.3->seqeval==1.2.2->-r /content/banglabert/requirements.txt (line 4)) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.3->seqeval==1.2.2->-r /content/banglabert/requirements.txt (line 4)) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets==1.11.0->-r /content/banglabert/requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets==1.11.0->-r /content/banglabert/requirements.txt (line 3)) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets==1.11.0->-r /content/banglabert/requirements.txt (line 3)) (1.15.0)\n",
            "Building wheels for collected packages: seqeval, normalizer, emoji, ftfy\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16179 sha256=c76b28591564ad3abbf7354e443584bed5c808052d5f6877850e0b08dc75a758\n",
            "  Stored in directory: /root/.cache/pip/wheels/ad/5c/ba/05fa33fa5855777b7d686e843ec07452f22a66a138e290e732\n",
            "  Building wheel for normalizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for normalizer: filename=normalizer-0.0.1-py3-none-any.whl size=6883 sha256=ddf0c2c96c5933ab37e07f9f0921b34c05f9ac865d743df0dca1e99e38b04430\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-60_zkkwq/wheels/8e/25/ac/a3666919774bd6e5a7818bc8630b4cfce6b3900f6299a261b6\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.4.2-py3-none-any.whl size=186469 sha256=521765395a2aea77b41e2af050ab502dcf568372260ee1ce1edd166fe6cf8593\n",
            "  Stored in directory: /root/.cache/pip/wheels/71/4d/3c/cada364d4ea0026deee7208dee1e61bcebd20aa2ae5dc154ba\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-6.0.3-py3-none-any.whl size=41933 sha256=89cd64854768dc2cd751c6d6e9b80f4d243d43881be703ce76d0b9cdd0d15edc\n",
            "  Stored in directory: /root/.cache/pip/wheels/7f/40/63/4bf603cec3ecc4a26985405834cb47eb8368bfa59e15dde046\n",
            "Successfully built seqeval normalizer emoji ftfy\n",
            "Installing collected packages: sentencepiece, emoji, xxhash, protobuf, multiprocess, ftfy, normalizer, huggingface-hub, seqeval, datasets\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.19.6\n",
            "    Uninstalling protobuf-3.19.6:\n",
            "      Successfully uninstalled protobuf-3.19.6\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.12.1\n",
            "    Uninstalling huggingface-hub-0.12.1:\n",
            "      Successfully uninstalled huggingface-hub-0.12.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.22.0 which is incompatible.\n",
            "tensorflow-metadata 1.12.0 requires protobuf<4,>=3.13, but you have protobuf 4.22.0 which is incompatible.\n",
            "tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.22.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-1.11.0 emoji-1.4.2 ftfy-6.0.3 huggingface-hub-0.0.19 multiprocess-0.70.14 normalizer-0.0.1 protobuf-4.22.0 sentencepiece-0.1.97 seqeval-1.2.2 xxhash-3.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade -r /content/banglabert/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "installing `protobuf(3.20)` for compatibility"
      ],
      "metadata": {
        "id": "XZG2dx6VIX-j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pplawdvNtLPk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11269ead-ece9-4110-fb16-b465d03518b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting protobuf==3.20\n",
            "  Downloading protobuf-3.20.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.22.0\n",
            "    Uninstalling protobuf-4.22.0:\n",
            "      Successfully uninstalled protobuf-4.22.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.0 which is incompatible.\n",
            "googleapis-common-protos 1.58.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-translate 3.8.4 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-language 2.6.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-firestore 2.7.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-datastore 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-bigquery 3.4.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.18.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-3.20.0\n"
          ]
        }
      ],
      "source": [
        "!pip install protobuf==3.20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aoLIfJ4-abl",
        "outputId": "4e482dde-8861-436e-c3f1-777114cd58ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ! rm -rf /content/data"
      ],
      "metadata": {
        "id": "Uacm7pqcKZQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1eH_GNjg-bb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d85a8f9-ea11-42cc-95a9-d066b2bec829"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^C\n"
          ]
        }
      ],
      "source": [
        "# copy the data from drive folder to content folder\n",
        "# ! cp -R /content/drive/MyDrive/GED\\ Data /content/data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data preprocessing and augmenting"
      ],
      "metadata": {
        "id": "2WhwEpdVIh-w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "GdcdFHPPIoYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bnlp_toolkit"
      ],
      "metadata": {
        "id": "82xHCit0Ilzr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d8ddef4-1c6a-4ff6-8a57-440f7e8791f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting bnlp_toolkit\n",
            "  Downloading bnlp_toolkit-3.2.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from bnlp_toolkit) (1.22.4)\n",
            "Collecting sklearn-crfsuite\n",
            "  Downloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.8/dist-packages (from bnlp_toolkit) (0.1.97)\n",
            "Requirement already satisfied: wasabi in /usr/local/lib/python3.8/dist-packages (from bnlp_toolkit) (0.10.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from bnlp_toolkit) (3.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from bnlp_toolkit) (4.64.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from bnlp_toolkit) (1.7.3)\n",
            "Collecting gensim==4.0.1\n",
            "  Downloading gensim-4.0.1-cp38-cp38-manylinux1_x86_64.whl (23.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.9/23.9 MB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.8/dist-packages (from gensim==4.0.1->bnlp_toolkit) (6.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk->bnlp_toolkit) (8.1.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk->bnlp_toolkit) (2022.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk->bnlp_toolkit) (1.2.0)\n",
            "Collecting python-crfsuite>=0.8.3\n",
            "  Downloading python_crfsuite-0.9.9-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.8/dist-packages (from sklearn-crfsuite->bnlp_toolkit) (0.8.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from sklearn-crfsuite->bnlp_toolkit) (1.15.0)\n",
            "Installing collected packages: python-crfsuite, sklearn-crfsuite, gensim, bnlp_toolkit\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed bnlp_toolkit-3.2.0 gensim-4.0.1 python-crfsuite-0.9.9 sklearn-crfsuite-0.3.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bnlp import POS\n",
        "bn_pos = POS()\n",
        "model_path = \"/content/data/bn_pos.pkl\"\n",
        "text = \"আমি ভাত খাই।\" # or you can pass ['আমি', 'ভাত', 'খাই', '।']\n",
        "res = bn_pos.tag(model_path, text)\n",
        "print(res)"
      ],
      "metadata": {
        "id": "2QfPXWREKKHc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "outputId": "175abfbb-dfb1-4171-e89a-52cd7abe1c2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "punkt not found. downloading...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "/usr/local/lib/python3.8/dist-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-e995d939fb90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/data/bn_pos.pkl\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"আমি ভাত খাই।\"\u001b[0m \u001b[0;31m# or you can pass ['আমি', 'ভাত', 'খাই', '।']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbn_pos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/bnlp/pos.py\u001b[0m in \u001b[0;36mtag\u001b[0;34m(self, model_path, text)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mPOS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpkl_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkl_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/data/bn_pos.pkl'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# csv_1 = pd.read_csv(\"/content/drive/MyDrive/GED Data/banglatribune.csv\",  encoding='utf-8', encoding_errors='ignore')\n",
        "csv_2 = pd.read_csv(\"/content/drive/MyDrive/GED Data/prothomalo.csv\",  encoding='utf-8', encoding_errors='ignore')\n",
        "# csv_3 = pd.read_csv(\"/content/drive/MyDrive/GED Data/globalvoices.csv\",  encoding='utf-8', encoding_errors='ignore')"
      ],
      "metadata": {
        "id": "pRbu3BAsIwFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df = pd.concat([csv_1, csv_2, csv_3],ignore_index=True)\n",
        "df = csv_2.sample(frac=1).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "TvePU-U_JqSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIWddZzDJC6C",
        "outputId": "ffc50b96-02ac-44c4-ed74-4a15501ddcf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "122268"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.sample(10000)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "dWJcoO7FQzwP",
        "outputId": "3f486e3e-9d38-486b-9754-b566f4f6185f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                    text\n",
              "82875  গোবিন্দগঞ্জ থানার ওসি মোজাম্মেল হক বলেন, স্থ...\n",
              "45918  দেশটির আইনপ্রণেতারা এ-সংক্রান্ত উদ্যোগের একটি ...\n",
              "85277              ছোট্ট শিশু মির্জা সাদিয়া তুত তায়বা।\n",
              "31783  জেমস বন্ড সিরিজের ২৪ তম ছবি স্পেকটর মুক্তি পাব...\n",
              "98987                           লিথোগ্রাফে অসাধারণ ছিলেন"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e6457afb-a0ca-4690-adb8-998edf9ec758\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>82875</th>\n",
              "      <td>গোবিন্দগঞ্জ থানার ওসি মোজাম্মেল হক বলেন, স্থ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45918</th>\n",
              "      <td>দেশটির আইনপ্রণেতারা এ-সংক্রান্ত উদ্যোগের একটি ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85277</th>\n",
              "      <td>ছোট্ট শিশু মির্জা সাদিয়া তুত তায়বা।</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31783</th>\n",
              "      <td>জেমস বন্ড সিরিজের ২৪ তম ছবি স্পেকটর মুক্তি পাব...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98987</th>\n",
              "      <td>লিথোগ্রাফে অসাধারণ ছিলেন</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e6457afb-a0ca-4690-adb8-998edf9ec758')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e6457afb-a0ca-4690-adb8-998edf9ec758 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e6457afb-a0ca-4690-adb8-998edf9ec758');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mmqf_uER5Kc",
        "outputId": "16b3ecca-2aaf-4088-d9ee-d7aa6e79603e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "dataset = []\n",
        "punct_list = [',', '?', '৷', ';']\n",
        "\n",
        "\n",
        "\n",
        "for index in tqdm(df.index):\n",
        "    positive = False\n",
        "    main_pos_tag = bn_pos.tag(model_path, str(df['text'][index]))\n",
        "    pos_tag = main_pos_tag.copy()\n",
        "    last_noun_index = -1\n",
        "    last_verb_index = -1\n",
        "    last_vaux_index = -1\n",
        "    last_nv_index = -1\n",
        "    last_lv_index = -1\n",
        "    last_jj_index = -1\n",
        "\n",
        "\n",
        "    for i in range(len(pos_tag)):\n",
        "        if pos_tag[i][1] == 'NC':\n",
        "            last_noun_index = i\n",
        "        if pos_tag[i][1] == 'VM':\n",
        "            last_verb_index = i\n",
        "        if pos_tag[i][1] == 'VAUX':\n",
        "            last_vaux_index = i\n",
        "        if pos_tag[i][1] == 'NV':\n",
        "            last_nv_index = i\n",
        "        if pos_tag[i][1] == 'LV':\n",
        "            last_lv_index = i\n",
        "        if pos_tag[i][1] == 'JJ':\n",
        "            last_jj_index = i\n",
        "\n",
        "    if np.random.uniform() < 0.5 :\n",
        "        wrong_sentence = ''\n",
        "        for i in range(len(pos_tag)):\n",
        "            if pos_tag[i][1] == 'PU':\n",
        "                punc = random.choice([char for char in punct_list if char != pos_tag[i][0]])\n",
        "                wrong_sentence = wrong_sentence + punc + \" \"\n",
        "            else:\n",
        "                wrong_sentence = wrong_sentence + pos_tag[i][0] + \" \"\n",
        "\n",
        "        if wrong_sentence != '':\n",
        "            wrong_sentence = wrong_sentence.strip()\n",
        "            # dataset.append({\"bn\": wrong_sentence, \"en\": df['text'][index]})\n",
        "            if not positive:\n",
        "                dataset.append({\"sentence1\": df['text'][index], \"label\": \"pos\"})\n",
        "                positive = True\n",
        "            dataset.append({\"sentence1\": wrong_sentence, \"label\": \"neg\"})\n",
        "\n",
        "    if last_noun_index != -1 and last_verb_index != -1:\n",
        "        pos_tag[last_noun_index], pos_tag[last_verb_index] = pos_tag[last_verb_index], pos_tag[last_noun_index]\n",
        "        wrong_sentence = ''\n",
        "        for word in pos_tag:\n",
        "            wrong_sentence = wrong_sentence + word[0] + \" \"\n",
        "\n",
        "\n",
        "        wrong_sentence = wrong_sentence.strip()\n",
        "        # dataset.append({\"bn\": wrong_sentence, \"en\": df['text'][index]})\n",
        "        if not positive:\n",
        "          dataset.append({\"sentence1\": df['text'][index], \"label\": \"pos\"})\n",
        "          positive = True\n",
        "        dataset.append({\"sentence1\": wrong_sentence, \"label\": \"neg\"})\n",
        "\n",
        "    pos_tag = main_pos_tag.copy()\n",
        "\n",
        "    if last_jj_index != -1 and last_noun_index != -1:\n",
        "        pos_tag[last_jj_index], pos_tag[last_noun_index] = pos_tag[last_noun_index], pos_tag[last_jj_index]\n",
        "        wrong_sentence = ''\n",
        "        for word in pos_tag:\n",
        "            wrong_sentence = wrong_sentence + word[0] + \" \"\n",
        "\n",
        "\n",
        "        wrong_sentence = wrong_sentence.strip()\n",
        "        # dataset.append({\"bn\": wrong_sentence, \"en\": df['text'][index]})\n",
        "        if not positive:\n",
        "          dataset.append({\"sentence1\": df['text'][index], \"label\": \"pos\"})\n",
        "          positive = True\n",
        "        dataset.append({\"sentence1\": wrong_sentence, \"label\": \"neg\"})\n",
        "\n",
        "    pos_tag = main_pos_tag.copy()\n",
        "#     last_vaux_index = -1\n",
        "\n",
        "#     for i in range(len(pos_tag)):\n",
        "#         if pos_tag[i][1] == 'VAUX':\n",
        "#             last_vaux_index = i\n",
        "\n",
        "    if last_vaux_index != -1 and last_vaux_index == len(pos_tag)-1:\n",
        "        wrong_sentence = ''\n",
        "        for i in range(len(pos_tag)-1):\n",
        "            wrong_sentence = wrong_sentence + pos_tag[i][0] + \" \"\n",
        "\n",
        "        wrong_sentence = wrong_sentence.strip()\n",
        "        # dataset.append({\"bn\": wrong_sentence, \"en\": df['text'][index]})\n",
        "        if not positive:\n",
        "          dataset.append({\"sentence1\": df['text'][index], \"label\": \"pos\"})\n",
        "          positive = True\n",
        "        dataset.append({\"sentence1\": wrong_sentence, \"label\": \"neg\"})\n",
        "\n",
        "\n",
        "    pos_tag = main_pos_tag.copy()\n",
        "#     last_nv_index = -1\n",
        "\n",
        "#     for i in range(len(pos_tag)):\n",
        "#         if pos_tag[i][1] == 'NV':\n",
        "#             last_nv_index = i\n",
        "\n",
        "    if last_nv_index != -1:\n",
        "        wrong_sentence = ''\n",
        "        for i in range(last_nv_index):\n",
        "            wrong_sentence = wrong_sentence + pos_tag[i][0] + \" \"\n",
        "\n",
        "        wrong_sentence = wrong_sentence.strip()\n",
        "        # dataset.append({\"bn\": wrong_sentence, \"en\": df['text'][index]})\n",
        "        if not positive:\n",
        "          dataset.append({\"sentence1\": df['text'][index], \"label\": \"pos\"})\n",
        "          positive = True\n",
        "        dataset.append({\"sentence1\": wrong_sentence, \"label\": \"neg\"})\n",
        "\n",
        "\n",
        "    pos_tag = main_pos_tag.copy()\n",
        "#     last_lv_index = -1\n",
        "\n",
        "#     for i in range(len(pos_tag)):\n",
        "#         if pos_tag[i][1] == 'LV':\n",
        "#             last_lv_index = i\n",
        "\n",
        "    if last_lv_index != -1:\n",
        "        wrong_sentence = ''\n",
        "        for i in range(last_lv_index):\n",
        "            wrong_sentence = wrong_sentence + pos_tag[i][0] + \" \"\n",
        "\n",
        "        wrong_sentence = wrong_sentence.strip()\n",
        "        # dataset.append({\"bn\": wrong_sentence, \"en\": df['text'][index]})\n",
        "        if not positive:\n",
        "          dataset.append({\"sentence1\": df['text'][index], \"label\": \"pos\"})\n",
        "          positive = True\n",
        "        dataset.append({\"sentence1\": wrong_sentence, \"label\": \"neg\"})\n",
        "\n",
        "\n",
        "\n",
        "    closest_vm_to_last_nv = -1\n",
        "    pos_tag = main_pos_tag.copy()\n",
        "\n",
        "    if last_nv_index != -1:\n",
        "        for i in range(last_nv_index, len(pos_tag)):\n",
        "            if pos_tag[i][1] == \"VM\":\n",
        "                closest_vm_to_last_nv = i\n",
        "                break\n",
        "\n",
        "        if closest_vm_to_last_nv != -1:\n",
        "            pos_tag[last_nv_index], pos_tag[closest_vm_to_last_nv] = pos_tag[closest_vm_to_last_nv], pos_tag[last_nv_index]\n",
        "\n",
        "            wrong_sentence = ''\n",
        "            for word in pos_tag:\n",
        "                wrong_sentence = wrong_sentence + word[0] + \" \"\n",
        "\n",
        "            wrong_sentence = wrong_sentence.strip()\n",
        "            # dataset.append({\"bn\": wrong_sentence, \"en\": df['text'][index]})\n",
        "            if not positive:\n",
        "              dataset.append({\"sentence1\": df['text'][index], \"label\": \"pos\"})\n",
        "            dataset.append({\"sentence1\": wrong_sentence, \"label\": \"neg\"})\n",
        "\n",
        "print(len(dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUC3pS4jJ0n6",
        "outputId": "24eca235-1657-48f3-e53d-723f2cc152bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [02:39<00:00, 62.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33696\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vAcbt3jTQ-MW",
        "outputId": "2845c805-1ce1-4dc1-c710-c2f029310a61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sentence1': 'জেমস বন্ড সিরিজের ২৪ তম ছবি স্পেকটর মুক্তি পাবে নভেম্বর ৬ আগামী ।',\n",
              " 'label': 'neg'}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile path_to_file.py"
      ],
      "metadata": {
        "id": "s41-QFgbWgM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnt = 0\n",
        "for line in dataset:\n",
        "  if line['label'] == 'neg':\n",
        "    cnt +=1\n",
        "cnt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYSZP3GXW9ay",
        "outputId": "ea6f67aa-9a1c-4335-e434-b8d6dd7931c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24040"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Randomly drop a portion of dataset to balance `neg` and `pos` tags"
      ],
      "metadata": {
        "id": "5s-GJ_PZTDYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_neg = [i for i in dataset if i['label'] == 'neg']\n",
        "dataset_pos = [i for i in dataset if i['label'] == 'pos']\n",
        "len(dataset_neg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdK6IM9YTCWU",
        "outputId": "a42deb79-42a7-4bbc-d256-034b5de14592"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24040"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "percentage_to_drop = 50\n",
        "dataset_dropped_neg = random.sample(dataset_neg, k=int(len(dataset_neg) * (100 - percentage_to_drop) / 100))"
      ],
      "metadata": {
        "id": "k07mI-G7T7H7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset_dropped_neg + dataset_pos"
      ],
      "metadata": {
        "id": "Un3FaxDCUIDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUcEA2YUUL38",
        "outputId": "cb9abec8-166b-4e2a-cb20-b67055a2c00d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21676"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.shuffle(dataset)"
      ],
      "metadata": {
        "id": "IkXX5WPcirIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = dataset[:int(len(dataset)*0.8)]\n",
        "valid_data = dataset[int(len(dataset)*0.8) : int(len(dataset)*0.85)]\n",
        "test_data = dataset[int(len(dataset)*0.85):]\n",
        "len(train_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8w8p4sL3itN5",
        "outputId": "06ba85f1-fdcb-4321-ffc2-8911d4fef49d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17340"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRkW3L8OjLZC",
        "outputId": "62ec5f4f-3c1f-45d4-8618-0c6e4e12692a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3252"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.DataFrame(train_data)\n",
        "valid_df = pd.DataFrame(valid_data)\n",
        "test_df = pd.DataFrame(test_data)"
      ],
      "metadata": {
        "id": "_zmRvQdzjPRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['label'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPsdNhyDUUGt",
        "outputId": "2c028174-f1e7-4eb8-e348-ebdb8b06d0a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "neg    9600\n",
              "pos    7740\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'train_df:{len(train_df)}, valid_df: {len(valid_df)}, test_df: {len(test_df)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZtGppd9j81R",
        "outputId": "f2288143-b971-4b60-b187-64f150a03853"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_df:17340, valid_df: 1084, test_df: 3252\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "drop error sentences"
      ],
      "metadata": {
        "id": "EloJ4ZOyltBG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = train_df.drop(index = train_df[train_df['sentence1'] == ''].index)\n",
        "valid_df = valid_df.drop(index = valid_df[valid_df['sentence1'] == ''].index)\n",
        "test_df = test_df.drop(index = test_df[test_df['sentence1'] == ''].index)"
      ],
      "metadata": {
        "id": "5ZQNsy1hjrEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'train_df:{len(train_df)}, valid_df: {len(valid_df)}, test_df: {len(test_df)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmtZeA69kJ_h",
        "outputId": "73f08af0-c384-4bc3-c89a-4c53cb2a2a3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_df:17336, valid_df: 1082, test_df: 3250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "save as `.csv`"
      ],
      "metadata": {
        "id": "hV0y0vWClpe-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# n = 20000\n",
        "# train_df = train_df.sample(int(n*0.8))\n",
        "# valid_df = valid_df.sample(int(n*0.05))\n",
        "# test_df = test_df.sample(int(n*0.15))"
      ],
      "metadata": {
        "id": "DNmG9dzZoUSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('data/train_bert_wp.csv', \"w\", encoding = \"utf-8\") as file:\n",
        "  train_df.to_csv(file, index = False)\n",
        "with open('data/validation_bert_wp.csv', \"w\", encoding = \"utf-8\") as file:\n",
        "  valid_df.to_csv(file, index = False)\n",
        "with open('data/test_bert_wp.csv', \"w\", encoding = \"utf-8\") as file:\n",
        "  test_df.to_csv(file, index = False)"
      ],
      "metadata": {
        "id": "aB9T5Q4MkO0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "copy to drive"
      ],
      "metadata": {
        "id": "fmXiFn4TllBy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ! cp -R /content/data /content/drive/MyDrive/GED\\ Data"
      ],
      "metadata": {
        "id": "XeHQKhixk71S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['label'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BaZPyhRdlxsW",
        "outputId": "a8614ea2-3f6c-4f4c-c214-0229c3d0cdda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "neg    9688\n",
              "pos    6312\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppleJWkrr2jk",
        "outputId": "157ff82d-3709-4900-9c30-30ed314b3291"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-28 17:54:58.047398: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-28 17:54:58.927675: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-28 17:54:58.927786: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-28 17:54:58.927806: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "02/28/2023 17:55:00 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "02/28/2023 17:55:00 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.EPOCH,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=2,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=outputs/runs/Feb28_17-55-00_5d5c455a313f,\n",
            "logging_first_step=False,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.EPOCH,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=5.0,\n",
            "output_dir=outputs/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=outputs,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=None,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=outputs/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.EPOCH,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.1,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.1,\n",
            ")\n",
            "02/28/2023 17:55:00 - WARNING - datasets.builder - Using custom data configuration default-035252c558c77007\n",
            "02/28/2023 17:55:00 - INFO - datasets.builder - Generating dataset csv (/root/.cache/huggingface/datasets/csv/default-035252c558c77007/0.0.0)\n",
            "Downloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/csv/default-035252c558c77007/0.0.0...\n",
            "02/28/2023 17:55:02 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\n",
            "100% 3/3 [00:00<00:00, 2362.99it/s]\n",
            "02/28/2023 17:55:02 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n",
            "02/28/2023 17:55:04 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n",
            "100% 3/3 [00:00<00:00, 96.22it/s]\n",
            "02/28/2023 17:55:04 - INFO - datasets.utils.info_utils - Unable to verify checksums.\n",
            "02/28/2023 17:55:04 - INFO - datasets.builder - Generating split train\n",
            "02/28/2023 17:55:05 - INFO - datasets.builder - Generating split validation\n",
            "02/28/2023 17:55:05 - INFO - datasets.builder - Generating split test\n",
            "02/28/2023 17:55:05 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-035252c558c77007/0.0.0. Subsequent calls will reuse this data.\n",
            "100% 3/3 [00:00<00:00, 50.96it/s]\n",
            "[INFO|file_utils.py:1665] 2023-02-28 17:55:06,110 >> https://huggingface.co/csebuetnlp/banglabert/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpxhx45mxz\n",
            "Downloading: 100% 586/586 [00:00<00:00, 403kB/s]\n",
            "[INFO|file_utils.py:1669] 2023-02-28 17:55:07,037 >> storing https://huggingface.co/csebuetnlp/banglabert/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/60928dc4b87f5881692890e6541e6538f91588d2ea40cbbbdc04cfb2cb83a6b1.2388211ba94f448fcf40aef3c9526142a8c2f2a8fb4fce8a3801462f51b2bab5\n",
            "[INFO|file_utils.py:1677] 2023-02-28 17:55:07,038 >> creating metadata file for /root/.cache/huggingface/transformers/60928dc4b87f5881692890e6541e6538f91588d2ea40cbbbdc04cfb2cb83a6b1.2388211ba94f448fcf40aef3c9526142a8c2f2a8fb4fce8a3801462f51b2bab5\n",
            "[INFO|configuration_utils.py:561] 2023-02-28 17:55:07,038 >> loading configuration file https://huggingface.co/csebuetnlp/banglabert/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/60928dc4b87f5881692890e6541e6538f91588d2ea40cbbbdc04cfb2cb83a6b1.2388211ba94f448fcf40aef3c9526142a8c2f2a8fb4fce8a3801462f51b2bab5\n",
            "[INFO|configuration_utils.py:598] 2023-02-28 17:55:07,041 >> Model config ElectraConfig {\n",
            "  \"architectures\": [\n",
            "    \"ElectraForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_size\": 768,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1665] 2023-02-28 17:55:07,970 >> https://huggingface.co/csebuetnlp/banglabert/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpkhbal96i\n",
            "Downloading: 100% 119/119 [00:00<00:00, 113kB/s]\n",
            "[INFO|file_utils.py:1669] 2023-02-28 17:55:08,892 >> storing https://huggingface.co/csebuetnlp/banglabert/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/76fa87a0ec9c34c9b15732bf7e06bced447feff46287b8e7d246a55d301784d7.b4f59cefeba4296760d2cf1037142788b96f2be40230bf6393d2fba714562485\n",
            "[INFO|file_utils.py:1677] 2023-02-28 17:55:08,892 >> creating metadata file for /root/.cache/huggingface/transformers/76fa87a0ec9c34c9b15732bf7e06bced447feff46287b8e7d246a55d301784d7.b4f59cefeba4296760d2cf1037142788b96f2be40230bf6393d2fba714562485\n",
            "[INFO|configuration_utils.py:561] 2023-02-28 17:55:09,797 >> loading configuration file https://huggingface.co/csebuetnlp/banglabert/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/60928dc4b87f5881692890e6541e6538f91588d2ea40cbbbdc04cfb2cb83a6b1.2388211ba94f448fcf40aef3c9526142a8c2f2a8fb4fce8a3801462f51b2bab5\n",
            "[INFO|configuration_utils.py:598] 2023-02-28 17:55:09,797 >> Model config ElectraConfig {\n",
            "  \"architectures\": [\n",
            "    \"ElectraForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_size\": 768,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1665] 2023-02-28 17:55:11,620 >> https://huggingface.co/csebuetnlp/banglabert/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpz_3rvnp1\n",
            "Downloading: 100% 528k/528k [00:00<00:00, 590kB/s]\n",
            "[INFO|file_utils.py:1669] 2023-02-28 17:55:13,452 >> storing https://huggingface.co/csebuetnlp/banglabert/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/65e95b847336b6bf69b37fdb8682a97e822799adcd9745dcf9bf44cfe4db1b9a.8f92ca2cf7e2eaa550b10c40331ae9bf0f2e40abe3b549f66a3d7f13bfc6de47\n",
            "[INFO|file_utils.py:1677] 2023-02-28 17:55:13,452 >> creating metadata file for /root/.cache/huggingface/transformers/65e95b847336b6bf69b37fdb8682a97e822799adcd9745dcf9bf44cfe4db1b9a.8f92ca2cf7e2eaa550b10c40331ae9bf0f2e40abe3b549f66a3d7f13bfc6de47\n",
            "[INFO|file_utils.py:1665] 2023-02-28 17:55:15,276 >> https://huggingface.co/csebuetnlp/banglabert/resolve/main/special_tokens_map.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpl4wx_6xj\n",
            "Downloading: 100% 112/112 [00:00<00:00, 104kB/s]\n",
            "[INFO|file_utils.py:1669] 2023-02-28 17:55:16,182 >> storing https://huggingface.co/csebuetnlp/banglabert/resolve/main/special_tokens_map.json in cache at /root/.cache/huggingface/transformers/7820dfc553e8dfb8a1e82042b7d0d691c7a7cd1e30ed2974218f696e81c5f3b1.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
            "[INFO|file_utils.py:1677] 2023-02-28 17:55:16,182 >> creating metadata file for /root/.cache/huggingface/transformers/7820dfc553e8dfb8a1e82042b7d0d691c7a7cd1e30ed2974218f696e81c5f3b1.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
            "[INFO|tokenization_utils_base.py:1739] 2023-02-28 17:55:18,007 >> loading file https://huggingface.co/csebuetnlp/banglabert/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/65e95b847336b6bf69b37fdb8682a97e822799adcd9745dcf9bf44cfe4db1b9a.8f92ca2cf7e2eaa550b10c40331ae9bf0f2e40abe3b549f66a3d7f13bfc6de47\n",
            "[INFO|tokenization_utils_base.py:1739] 2023-02-28 17:55:18,008 >> loading file https://huggingface.co/csebuetnlp/banglabert/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1739] 2023-02-28 17:55:18,008 >> loading file https://huggingface.co/csebuetnlp/banglabert/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/7820dfc553e8dfb8a1e82042b7d0d691c7a7cd1e30ed2974218f696e81c5f3b1.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
            "[INFO|tokenization_utils_base.py:1739] 2023-02-28 17:55:18,008 >> loading file https://huggingface.co/csebuetnlp/banglabert/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/76fa87a0ec9c34c9b15732bf7e06bced447feff46287b8e7d246a55d301784d7.b4f59cefeba4296760d2cf1037142788b96f2be40230bf6393d2fba714562485\n",
            "[INFO|tokenization_utils_base.py:1739] 2023-02-28 17:55:18,008 >> loading file https://huggingface.co/csebuetnlp/banglabert/resolve/main/tokenizer.json from cache at None\n",
            "[INFO|configuration_utils.py:561] 2023-02-28 17:55:18,927 >> loading configuration file https://huggingface.co/csebuetnlp/banglabert/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/60928dc4b87f5881692890e6541e6538f91588d2ea40cbbbdc04cfb2cb83a6b1.2388211ba94f448fcf40aef3c9526142a8c2f2a8fb4fce8a3801462f51b2bab5\n",
            "[INFO|configuration_utils.py:598] 2023-02-28 17:55:18,928 >> Model config ElectraConfig {\n",
            "  \"architectures\": [\n",
            "    \"ElectraForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_size\": 768,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1665] 2023-02-28 17:55:19,956 >> https://huggingface.co/csebuetnlp/banglabert/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpeyrls42l\n",
            "Downloading: 100% 443M/443M [00:06<00:00, 70.1MB/s]\n",
            "[INFO|file_utils.py:1669] 2023-02-28 17:55:27,179 >> storing https://huggingface.co/csebuetnlp/banglabert/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/913ea71768a80ccdde3a9ab9a88cf2a93f37a52008896997655d0f63b0d0743a.8aaedac281b72dbb5296319c53be5a4e4a52339eded3f68d49201e140e221615\n",
            "[INFO|file_utils.py:1677] 2023-02-28 17:55:27,179 >> creating metadata file for /root/.cache/huggingface/transformers/913ea71768a80ccdde3a9ab9a88cf2a93f37a52008896997655d0f63b0d0743a.8aaedac281b72dbb5296319c53be5a4e4a52339eded3f68d49201e140e221615\n",
            "[INFO|modeling_utils.py:1279] 2023-02-28 17:55:27,179 >> loading weights file https://huggingface.co/csebuetnlp/banglabert/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/913ea71768a80ccdde3a9ab9a88cf2a93f37a52008896997655d0f63b0d0743a.8aaedac281b72dbb5296319c53be5a4e4a52339eded3f68d49201e140e221615\n",
            "[WARNING|modeling_utils.py:1515] 2023-02-28 17:55:28,794 >> Some weights of the model checkpoint at csebuetnlp/banglabert were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias']\n",
            "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:1526] 2023-02-28 17:55:28,794 >> Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at csebuetnlp/banglabert and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "02/28/2023 17:55:28 - WARNING - datasets.fingerprint - Parameter 'function'=<function main.<locals>.normalize_example at 0x7fd37deec1f0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "Running normalization on dataset:   0% 0/17336 [00:00<?, ?ex/s]02/28/2023 17:55:28 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-035252c558c77007/0.0.0/cache-1c80317fa3b1799d.arrow\n",
            "Running normalization on dataset: 100% 17336/17336 [00:03<00:00, 4427.98ex/s]\n",
            "02/28/2023 17:55:32 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.normalize_example at 0x7fd37deec1f0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n",
            "Running normalization on dataset:   0% 0/1082 [00:00<?, ?ex/s]02/28/2023 17:55:32 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-035252c558c77007/0.0.0/cache-bdd640fb06671ad1.arrow\n",
            "Running normalization on dataset: 100% 1082/1082 [00:00<00:00, 4291.22ex/s]\n",
            "02/28/2023 17:55:32 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.normalize_example at 0x7fd37deec1f0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n",
            "Running normalization on dataset:   0% 0/3250 [00:00<?, ?ex/s]02/28/2023 17:55:32 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-035252c558c77007/0.0.0/cache-3eb13b9046685257.arrow\n",
            "Running normalization on dataset: 100% 3250/3250 [00:00<00:00, 4399.81ex/s]\n",
            "02/28/2023 17:55:33 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.preprocess_function at 0x7fd37e43ae50> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n",
            "Running tokenizer on dataset:   0% 0/18 [00:00<?, ?ba/s]02/28/2023 17:55:33 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-035252c558c77007/0.0.0/cache-23b8c1e9392456de.arrow\n",
            "Running tokenizer on dataset: 100% 18/18 [00:06<00:00,  2.90ba/s]\n",
            "02/28/2023 17:55:39 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.preprocess_function at 0x7fd37e43ae50> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n",
            "Running tokenizer on dataset:   0% 0/2 [00:00<?, ?ba/s]02/28/2023 17:55:40 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-035252c558c77007/0.0.0/cache-1a3d1fa7bc8960a9.arrow\n",
            "Running tokenizer on dataset: 100% 2/2 [00:00<00:00,  4.58ba/s]\n",
            "02/28/2023 17:55:40 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.preprocess_function at 0x7fd37e43ae50> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n",
            "Running tokenizer on dataset:   0% 0/4 [00:00<?, ?ba/s]02/28/2023 17:55:40 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-035252c558c77007/0.0.0/cache-bd9c66b3ad3c2d6d.arrow\n",
            "Running tokenizer on dataset: 100% 4/4 [00:01<00:00,  2.67ba/s]\n",
            "02/28/2023 17:55:41 - INFO - __main__ - Sample 2848 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'input_ids': [2, 925, 2036, 5310, 795, 902, 1192, 2499, 4170, 22608, 2229, 2776, 4140, 31406, 764, 1886, 990, 14978, 792, 5937, 205, 3], 'label': 0, 'sentence1': 'কিন্তু তাতে সাড়া না দিয়ে গত ১৪ অক্টোবর মন্ত্রিপরিষদ বিভাগ উপজেলা পরিষদের ক্ষমতায়নের নামে একটি স্মারক করে জারি ।', 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "02/28/2023 17:55:41 - INFO - __main__ - Sample 13825 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'input_ids': [2, 15206, 8451, 949, 2501, 2107, 8829, 9248, 1815, 972, 205, 3], 'label': 1, 'sentence1': 'লোকটির সামনেই একটা পুলিশের গাড়ি লাইট জ্বালিয়ে দাঁড়িয়ে আছে।', 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "02/28/2023 17:55:41 - INFO - __main__ - Sample 1041 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'input_ids': [2, 2506, 5386, 5966, 5880, 1258, 12163, 888, 965, 17, 2295, 842, 1849, 5, 1811, 6274, 1257, 770, 13406, 763, 28695, 1954, 792, 4325, 1105, 2445, 2445, 824, 3914, 965, 205, 3], 'label': 0, 'sentence1': 'অথচ সেসব স্বপ্নের ঘোর আমাকে ভাসিয়ে নিয়ে যায় - কোথা থেকে কোথায় ! বাইরে পরাবাস্তবতার ডানায় ভর করে বিকেলগুলো ধীরে ধীরে হয়ে সন্ধ্যা যায় ।', 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "02/28/2023 17:55:43 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/accuracy/accuracy.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpab2zpz6h\n",
            "Downloading: 3.21kB [00:00, 2.97MB/s]       \n",
            "02/28/2023 17:55:43 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/accuracy/accuracy.py in cache at /root/.cache/huggingface/datasets/downloads/591d385c0492ec80bad3323a081d7b1126c681a389acd9759f0a3098d2034ceb.1d0bd855d8e3c9eb14daa00edb910a511da1d653aeca8e7bc5dffb23a84d7b54.py\n",
            "02/28/2023 17:55:43 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/591d385c0492ec80bad3323a081d7b1126c681a389acd9759f0a3098d2034ceb.1d0bd855d8e3c9eb14daa00edb910a511da1d653aeca8e7bc5dffb23a84d7b54.py\n",
            "02/28/2023 17:55:43 - INFO - datasets.load - Creating main folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/accuracy/accuracy.py at /root/.cache/huggingface/modules/datasets_modules/metrics/accuracy\n",
            "02/28/2023 17:55:43 - INFO - datasets.load - Creating specific version folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/accuracy/accuracy.py at /root/.cache/huggingface/modules/datasets_modules/metrics/accuracy/6dba4616f6b2bbd19659d1db3a48cc001c8f13a10cdc73a2641a55f7a60b7b5b\n",
            "02/28/2023 17:55:43 - INFO - datasets.load - Copying script file from https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/accuracy/accuracy.py to /root/.cache/huggingface/modules/datasets_modules/metrics/accuracy/6dba4616f6b2bbd19659d1db3a48cc001c8f13a10cdc73a2641a55f7a60b7b5b/accuracy.py\n",
            "02/28/2023 17:55:43 - INFO - datasets.load - Couldn't find dataset infos file at https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/accuracy/dataset_infos.json\n",
            "02/28/2023 17:55:43 - INFO - datasets.load - Creating metadata file for metric https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/accuracy/accuracy.py at /root/.cache/huggingface/modules/datasets_modules/metrics/accuracy/6dba4616f6b2bbd19659d1db3a48cc001c8f13a10cdc73a2641a55f7a60b7b5b/accuracy.json\n",
            "02/28/2023 17:55:44 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/precision/precision.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp9xx5ngkd\n",
            "Downloading: 4.75kB [00:00, 4.58MB/s]       \n",
            "02/28/2023 17:55:44 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/precision/precision.py in cache at /root/.cache/huggingface/datasets/downloads/06a102f46e97d31731c2b6217107bb99bc926f4c01d25b7879834bb087526c9c.c43ed2cac512b5b9b0022ec45841dc6d3a5498ec180e190caf9170b8ca6d83eb.py\n",
            "02/28/2023 17:55:44 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/06a102f46e97d31731c2b6217107bb99bc926f4c01d25b7879834bb087526c9c.c43ed2cac512b5b9b0022ec45841dc6d3a5498ec180e190caf9170b8ca6d83eb.py\n",
            "02/28/2023 17:55:45 - INFO - datasets.load - Creating main folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/precision/precision.py at /root/.cache/huggingface/modules/datasets_modules/metrics/precision\n",
            "02/28/2023 17:55:45 - INFO - datasets.load - Creating specific version folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/precision/precision.py at /root/.cache/huggingface/modules/datasets_modules/metrics/precision/94709a71c6fe37171ef49d3466fec24dee9a79846c9f176dff66a649e9811690\n",
            "02/28/2023 17:55:45 - INFO - datasets.load - Copying script file from https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/precision/precision.py to /root/.cache/huggingface/modules/datasets_modules/metrics/precision/94709a71c6fe37171ef49d3466fec24dee9a79846c9f176dff66a649e9811690/precision.py\n",
            "02/28/2023 17:55:45 - INFO - datasets.load - Couldn't find dataset infos file at https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/precision/dataset_infos.json\n",
            "02/28/2023 17:55:45 - INFO - datasets.load - Creating metadata file for metric https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/precision/precision.py at /root/.cache/huggingface/modules/datasets_modules/metrics/precision/94709a71c6fe37171ef49d3466fec24dee9a79846c9f176dff66a649e9811690/precision.json\n",
            "02/28/2023 17:55:46 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/recall/recall.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpt5dp6mh2\n",
            "Downloading: 4.73kB [00:00, 4.02MB/s]       \n",
            "02/28/2023 17:55:46 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/recall/recall.py in cache at /root/.cache/huggingface/datasets/downloads/9d1e5a9da095d761417042b1cdd771625ce1377878c123f23ce3be6cdb09e30e.49bf02ab37c819ce8e2e3d98b5b41b3b4aaa77dec6888247ee015c38a6cf901e.py\n",
            "02/28/2023 17:55:46 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/9d1e5a9da095d761417042b1cdd771625ce1377878c123f23ce3be6cdb09e30e.49bf02ab37c819ce8e2e3d98b5b41b3b4aaa77dec6888247ee015c38a6cf901e.py\n",
            "02/28/2023 17:55:46 - INFO - datasets.load - Creating main folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/recall/recall.py at /root/.cache/huggingface/modules/datasets_modules/metrics/recall\n",
            "02/28/2023 17:55:46 - INFO - datasets.load - Creating specific version folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/recall/recall.py at /root/.cache/huggingface/modules/datasets_modules/metrics/recall/1e3b93e2bed42e1498e628f161d79ee019dd8e78d36985d3c7ecbc018adf35e8\n",
            "02/28/2023 17:55:46 - INFO - datasets.load - Copying script file from https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/recall/recall.py to /root/.cache/huggingface/modules/datasets_modules/metrics/recall/1e3b93e2bed42e1498e628f161d79ee019dd8e78d36985d3c7ecbc018adf35e8/recall.py\n",
            "02/28/2023 17:55:46 - INFO - datasets.load - Couldn't find dataset infos file at https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/recall/dataset_infos.json\n",
            "02/28/2023 17:55:46 - INFO - datasets.load - Creating metadata file for metric https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/recall/recall.py at /root/.cache/huggingface/modules/datasets_modules/metrics/recall/1e3b93e2bed42e1498e628f161d79ee019dd8e78d36985d3c7ecbc018adf35e8/recall.json\n",
            "02/28/2023 17:55:48 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/f1/f1.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpd757313d\n",
            "Downloading: 4.64kB [00:00, 3.24MB/s]       \n",
            "02/28/2023 17:55:48 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/f1/f1.py in cache at /root/.cache/huggingface/datasets/downloads/6d52348ec3313cb4ee55fa1f25ada40bb6b32b0092fdae9d7013317b57d93d2a.304aeb02554661198ae8e7c1f757922f09a7224740649a7e04b7a71ded92eebf.py\n",
            "02/28/2023 17:55:48 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/6d52348ec3313cb4ee55fa1f25ada40bb6b32b0092fdae9d7013317b57d93d2a.304aeb02554661198ae8e7c1f757922f09a7224740649a7e04b7a71ded92eebf.py\n",
            "02/28/2023 17:55:48 - INFO - datasets.load - Creating main folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/f1/f1.py at /root/.cache/huggingface/modules/datasets_modules/metrics/f1\n",
            "02/28/2023 17:55:48 - INFO - datasets.load - Creating specific version folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/f1/f1.py at /root/.cache/huggingface/modules/datasets_modules/metrics/f1/6c86fddbf90432b9c43a8c38c62a0dd9de63bad2ef0a896f9ae20273d6d6f6e9\n",
            "02/28/2023 17:55:48 - INFO - datasets.load - Copying script file from https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/f1/f1.py to /root/.cache/huggingface/modules/datasets_modules/metrics/f1/6c86fddbf90432b9c43a8c38c62a0dd9de63bad2ef0a896f9ae20273d6d6f6e9/f1.py\n",
            "02/28/2023 17:55:48 - INFO - datasets.load - Couldn't find dataset infos file at https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/f1/dataset_infos.json\n",
            "02/28/2023 17:55:48 - INFO - datasets.load - Creating metadata file for metric https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/f1/f1.py at /root/.cache/huggingface/modules/datasets_modules/metrics/f1/6c86fddbf90432b9c43a8c38c62a0dd9de63bad2ef0a896f9ae20273d6d6f6e9/f1.json\n",
            "[INFO|trainer.py:520] 2023-02-28 17:55:51,749 >> The following columns in the training set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: sentence1.\n",
            "[INFO|trainer.py:1168] 2023-02-28 17:55:51,756 >> ***** Running training *****\n",
            "[INFO|trainer.py:1169] 2023-02-28 17:55:51,756 >>   Num examples = 17336\n",
            "[INFO|trainer.py:1170] 2023-02-28 17:55:51,756 >>   Num Epochs = 5\n",
            "[INFO|trainer.py:1171] 2023-02-28 17:55:51,756 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1172] 2023-02-28 17:55:51,756 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1173] 2023-02-28 17:55:51,756 >>   Gradient Accumulation steps = 2\n",
            "[INFO|trainer.py:1174] 2023-02-28 17:55:51,756 >>   Total optimization steps = 5415\n",
            "{'loss': 0.2977, 'learning_rate': 1.7779601887954035e-05, 'epoch': 1.0}\n",
            " 20% 1083/5415 [02:43<10:00,  7.22it/s][INFO|trainer.py:520] 2023-02-28 17:58:34,807 >> The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: sentence1.\n",
            "[INFO|trainer.py:2181] 2023-02-28 17:58:34,809 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2183] 2023-02-28 17:58:34,809 >>   Num examples = 1082\n",
            "[INFO|trainer.py:2186] 2023-02-28 17:58:34,809 >>   Batch size = 8\n",
            "\n",
            "  0% 0/136 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 7/136 [00:00<00:02, 60.21it/s]\u001b[A\n",
            " 10% 14/136 [00:00<00:02, 56.72it/s]\u001b[A\n",
            " 15% 20/136 [00:00<00:02, 54.48it/s]\u001b[A\n",
            " 19% 26/136 [00:00<00:02, 53.55it/s]\u001b[A\n",
            " 24% 32/136 [00:00<00:02, 49.46it/s]\u001b[A\n",
            " 28% 38/136 [00:00<00:01, 50.61it/s]\u001b[A\n",
            " 32% 44/136 [00:00<00:01, 51.12it/s]\u001b[A\n",
            " 37% 50/136 [00:00<00:01, 51.91it/s]\u001b[A\n",
            " 41% 56/136 [00:01<00:01, 49.46it/s]\u001b[A\n",
            " 45% 61/136 [00:01<00:01, 48.01it/s]\u001b[A\n",
            " 49% 67/136 [00:01<00:01, 49.45it/s]\u001b[A\n",
            " 53% 72/136 [00:01<00:01, 48.27it/s]\u001b[A\n",
            " 57% 77/136 [00:01<00:01, 43.46it/s]\u001b[A\n",
            " 60% 82/136 [00:01<00:01, 45.03it/s]\u001b[A\n",
            " 65% 88/136 [00:01<00:01, 47.18it/s]\u001b[A\n",
            " 69% 94/136 [00:01<00:00, 49.98it/s]\u001b[A\n",
            " 74% 100/136 [00:02<00:00, 48.89it/s]\u001b[A\n",
            " 77% 105/136 [00:02<00:00, 49.10it/s]\u001b[A\n",
            " 81% 110/136 [00:02<00:00, 49.18it/s]\u001b[A\n",
            " 85% 116/136 [00:02<00:00, 49.67it/s]\u001b[A\n",
            " 90% 122/136 [00:02<00:00, 50.80it/s]\u001b[A\n",
            " 94% 128/136 [00:02<00:00, 47.82it/s]\u001b[A\n",
            " 98% 133/136 [00:02<00:00, 47.87it/s]\u001b[A02/28/2023 17:58:37 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
            "02/28/2023 17:58:37 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow\n",
            "02/28/2023 17:58:37 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow\n",
            "02/28/2023 17:58:37 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow\n",
            "\n",
            "{'eval_loss': 0.23701085150241852, 'eval_accuracy': 0.9260628465804066, 'eval_precision': 0.9260628465804066, 'eval_recall': 0.9329021686680067, 'eval_f1': 0.9257696595489238, 'eval_runtime': 2.805, 'eval_samples_per_second': 385.74, 'eval_steps_per_second': 48.485, 'epoch': 1.0}\n",
            "\n",
            " 20% 1083/5415 [02:45<10:00,  7.22it/s]\n",
            "                                     \u001b[A[INFO|trainer.py:1935] 2023-02-28 17:58:37,616 >> Saving model checkpoint to outputs/checkpoint-1083\n",
            "[INFO|configuration_utils.py:391] 2023-02-28 17:58:37,616 >> Configuration saved in outputs/checkpoint-1083/config.json\n",
            "[INFO|modeling_utils.py:1001] 2023-02-28 17:58:39,065 >> Model weights saved in outputs/checkpoint-1083/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2020] 2023-02-28 17:58:39,066 >> tokenizer config file saved in outputs/checkpoint-1083/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2026] 2023-02-28 17:58:39,066 >> Special tokens file saved in outputs/checkpoint-1083/special_tokens_map.json\n",
            "{'loss': 0.1546, 'learning_rate': 1.3334701415965525e-05, 'epoch': 2.0}\n",
            " 40% 2166/5415 [05:36<08:55,  6.07it/s][INFO|trainer.py:520] 2023-02-28 18:01:28,743 >> The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: sentence1.\n",
            "[INFO|trainer.py:2181] 2023-02-28 18:01:28,746 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2183] 2023-02-28 18:01:28,746 >>   Num examples = 1082\n",
            "[INFO|trainer.py:2186] 2023-02-28 18:01:28,746 >>   Batch size = 8\n",
            "\n",
            "  0% 0/136 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 6/136 [00:00<00:02, 58.84it/s]\u001b[A\n",
            "  9% 12/136 [00:00<00:02, 52.24it/s]\u001b[A\n",
            " 13% 18/136 [00:00<00:02, 53.59it/s]\u001b[A\n",
            " 18% 24/136 [00:00<00:02, 50.20it/s]\u001b[A\n",
            " 22% 30/136 [00:00<00:02, 47.33it/s]\u001b[A\n",
            " 26% 35/136 [00:00<00:02, 47.86it/s]\u001b[A\n",
            " 30% 41/136 [00:00<00:01, 48.63it/s]\u001b[A\n",
            " 35% 47/136 [00:00<00:01, 49.57it/s]\u001b[A\n",
            " 38% 52/136 [00:01<00:01, 48.65it/s]\u001b[A\n",
            " 42% 57/136 [00:01<00:01, 44.84it/s]\u001b[A\n",
            " 46% 62/136 [00:01<00:01, 45.64it/s]\u001b[A\n",
            " 49% 67/136 [00:01<00:01, 45.42it/s]\u001b[A\n",
            " 53% 72/136 [00:01<00:01, 44.83it/s]\u001b[A\n",
            " 57% 77/136 [00:01<00:01, 40.74it/s]\u001b[A\n",
            " 60% 82/136 [00:01<00:01, 42.96it/s]\u001b[A\n",
            " 65% 88/136 [00:01<00:01, 45.61it/s]\u001b[A\n",
            " 69% 94/136 [00:01<00:00, 48.01it/s]\u001b[A\n",
            " 73% 99/136 [00:02<00:00, 46.38it/s]\u001b[A\n",
            " 76% 104/136 [00:02<00:00, 47.22it/s]\u001b[A\n",
            " 80% 109/136 [00:02<00:00, 47.81it/s]\u001b[A\n",
            " 84% 114/136 [00:02<00:00, 47.86it/s]\u001b[A\n",
            " 88% 120/136 [00:02<00:00, 49.09it/s]\u001b[A\n",
            " 92% 125/136 [00:02<00:00, 45.71it/s]\u001b[A\n",
            " 96% 130/136 [00:02<00:00, 46.24it/s]\u001b[A\n",
            " 99% 135/136 [00:02<00:00, 45.64it/s]\u001b[A02/28/2023 18:01:31 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
            "02/28/2023 18:01:31 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow\n",
            "02/28/2023 18:01:31 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow\n",
            "02/28/2023 18:01:31 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow\n",
            "\n",
            "{'eval_loss': 0.22326445579528809, 'eval_accuracy': 0.9362292051756007, 'eval_precision': 0.9339797430830039, 'eval_recall': 0.9391005946739708, 'eval_f1': 0.9356460350913487, 'eval_runtime': 2.9859, 'eval_samples_per_second': 362.376, 'eval_steps_per_second': 45.548, 'epoch': 2.0}\n",
            "\n",
            " 40% 2166/5415 [05:39<08:55,  6.07it/s]\n",
            "                                     \u001b[A[INFO|trainer.py:1935] 2023-02-28 18:01:31,734 >> Saving model checkpoint to outputs/checkpoint-2166\n",
            "[INFO|configuration_utils.py:391] 2023-02-28 18:01:31,735 >> Configuration saved in outputs/checkpoint-2166/config.json\n",
            "[INFO|modeling_utils.py:1001] 2023-02-28 18:01:33,268 >> Model weights saved in outputs/checkpoint-2166/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2020] 2023-02-28 18:01:33,268 >> tokenizer config file saved in outputs/checkpoint-2166/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2026] 2023-02-28 18:01:33,269 >> Special tokens file saved in outputs/checkpoint-2166/special_tokens_map.json\n",
            "{'loss': 0.121, 'learning_rate': 8.889800943977017e-06, 'epoch': 3.0}\n",
            " 60% 3249/5415 [08:30<06:41,  5.39it/s][INFO|trainer.py:520] 2023-02-28 18:04:22,530 >> The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: sentence1.\n",
            "[INFO|trainer.py:2181] 2023-02-28 18:04:22,532 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2183] 2023-02-28 18:04:22,532 >>   Num examples = 1082\n",
            "[INFO|trainer.py:2186] 2023-02-28 18:04:22,532 >>   Batch size = 8\n",
            "\n",
            "  0% 0/136 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 6/136 [00:00<00:02, 59.21it/s]\u001b[A\n",
            "  9% 12/136 [00:00<00:02, 53.96it/s]\u001b[A\n",
            " 13% 18/136 [00:00<00:02, 56.06it/s]\u001b[A\n",
            " 18% 24/136 [00:00<00:02, 51.69it/s]\u001b[A\n",
            " 22% 30/136 [00:00<00:02, 49.11it/s]\u001b[A\n",
            " 26% 36/136 [00:00<00:01, 50.27it/s]\u001b[A\n",
            " 31% 42/136 [00:00<00:01, 50.50it/s]\u001b[A\n",
            " 35% 48/136 [00:00<00:01, 51.66it/s]\u001b[A\n",
            " 40% 54/136 [00:01<00:01, 50.66it/s]\u001b[A\n",
            " 44% 60/136 [00:01<00:01, 47.08it/s]\u001b[A\n",
            " 49% 66/136 [00:01<00:01, 48.85it/s]\u001b[A\n",
            " 52% 71/136 [00:01<00:01, 48.38it/s]\u001b[A\n",
            " 56% 76/136 [00:01<00:01, 43.42it/s]\u001b[A\n",
            " 60% 81/136 [00:01<00:01, 44.42it/s]\u001b[A\n",
            " 64% 87/136 [00:01<00:01, 46.66it/s]\u001b[A\n",
            " 68% 93/136 [00:01<00:00, 49.17it/s]\u001b[A\n",
            " 72% 98/136 [00:01<00:00, 48.84it/s]\u001b[A\n",
            " 76% 104/136 [00:02<00:00, 49.67it/s]\u001b[A\n",
            " 81% 110/136 [00:02<00:00, 49.88it/s]\u001b[A\n",
            " 85% 116/136 [00:02<00:00, 50.51it/s]\u001b[A\n",
            " 90% 122/136 [00:02<00:00, 51.05it/s]\u001b[A\n",
            " 94% 128/136 [00:02<00:00, 47.02it/s]\u001b[A\n",
            " 98% 133/136 [00:02<00:00, 46.31it/s]\u001b[A02/28/2023 18:04:25 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
            "02/28/2023 18:04:25 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow\n",
            "02/28/2023 18:04:25 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow\n",
            "02/28/2023 18:04:25 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow\n",
            "\n",
            "{'eval_loss': 0.2379399687051773, 'eval_accuracy': 0.9288354898336414, 'eval_precision': 0.9275722021660651, 'eval_recall': 0.9341849009050291, 'eval_f1': 0.9284344202658905, 'eval_runtime': 2.8397, 'eval_samples_per_second': 381.031, 'eval_steps_per_second': 47.893, 'epoch': 3.0}\n",
            "\n",
            " 60% 3249/5415 [08:33<06:41,  5.39it/s]\n",
            "                                     \u001b[A[INFO|trainer.py:1935] 2023-02-28 18:04:25,374 >> Saving model checkpoint to outputs/checkpoint-3249\n",
            "[INFO|configuration_utils.py:391] 2023-02-28 18:04:25,374 >> Configuration saved in outputs/checkpoint-3249/config.json\n",
            "[INFO|modeling_utils.py:1001] 2023-02-28 18:04:26,754 >> Model weights saved in outputs/checkpoint-3249/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2020] 2023-02-28 18:04:26,755 >> tokenizer config file saved in outputs/checkpoint-3249/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2026] 2023-02-28 18:04:26,756 >> Special tokens file saved in outputs/checkpoint-3249/special_tokens_map.json\n",
            "{'loss': 0.1012, 'learning_rate': 4.444900471988509e-06, 'epoch': 4.0}\n",
            " 80% 4332/5415 [11:24<02:46,  6.52it/s][INFO|trainer.py:520] 2023-02-28 18:07:16,746 >> The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: sentence1.\n",
            "[INFO|trainer.py:2181] 2023-02-28 18:07:16,748 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2183] 2023-02-28 18:07:16,748 >>   Num examples = 1082\n",
            "[INFO|trainer.py:2186] 2023-02-28 18:07:16,748 >>   Batch size = 8\n",
            "\n",
            "  0% 0/136 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 7/136 [00:00<00:02, 60.28it/s]\u001b[A\n",
            " 10% 14/136 [00:00<00:02, 56.48it/s]\u001b[A\n",
            " 15% 20/136 [00:00<00:02, 54.37it/s]\u001b[A\n",
            " 19% 26/136 [00:00<00:02, 53.22it/s]\u001b[A\n",
            " 24% 32/136 [00:00<00:02, 49.75it/s]\u001b[A\n",
            " 28% 38/136 [00:00<00:01, 50.59it/s]\u001b[A\n",
            " 32% 44/136 [00:00<00:01, 50.89it/s]\u001b[A\n",
            " 37% 50/136 [00:00<00:01, 51.30it/s]\u001b[A\n",
            " 41% 56/136 [00:01<00:01, 48.35it/s]\u001b[A\n",
            " 45% 61/136 [00:01<00:01, 47.32it/s]\u001b[A\n",
            " 49% 67/136 [00:01<00:01, 48.94it/s]\u001b[A\n",
            " 53% 72/136 [00:01<00:01, 47.98it/s]\u001b[A\n",
            " 57% 77/136 [00:01<00:01, 42.75it/s]\u001b[A\n",
            " 61% 83/136 [00:01<00:01, 45.06it/s]\u001b[A\n",
            " 65% 89/136 [00:01<00:00, 47.33it/s]\u001b[A\n",
            " 70% 95/136 [00:01<00:00, 49.94it/s]\u001b[A\n",
            " 74% 101/136 [00:02<00:00, 49.20it/s]\u001b[A\n",
            " 78% 106/136 [00:02<00:00, 49.16it/s]\u001b[A\n",
            " 82% 112/136 [00:02<00:00, 49.85it/s]\u001b[A\n",
            " 87% 118/136 [00:02<00:00, 50.63it/s]\u001b[A\n",
            " 91% 124/136 [00:02<00:00, 48.97it/s]\u001b[A\n",
            " 95% 129/136 [00:02<00:00, 47.05it/s]\u001b[A\n",
            " 99% 134/136 [00:02<00:00, 47.02it/s]\u001b[A02/28/2023 18:07:19 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
            "02/28/2023 18:07:19 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow\n",
            "02/28/2023 18:07:19 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow\n",
            "02/28/2023 18:07:19 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow\n",
            "\n",
            "{'eval_loss': 0.2935919761657715, 'eval_accuracy': 0.9325323475046211, 'eval_precision': 0.9302673473869126, 'eval_recall': 0.9353443936443134, 'eval_f1': 0.9319153704589631, 'eval_runtime': 2.8322, 'eval_samples_per_second': 382.04, 'eval_steps_per_second': 48.02, 'epoch': 4.0}\n",
            "\n",
            " 80% 4332/5415 [11:27<02:46,  6.52it/s]\n",
            "                                     \u001b[A[INFO|trainer.py:1935] 2023-02-28 18:07:19,582 >> Saving model checkpoint to outputs/checkpoint-4332\n",
            "[INFO|configuration_utils.py:391] 2023-02-28 18:07:19,583 >> Configuration saved in outputs/checkpoint-4332/config.json\n",
            "[INFO|modeling_utils.py:1001] 2023-02-28 18:07:20,991 >> Model weights saved in outputs/checkpoint-4332/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2020] 2023-02-28 18:07:20,992 >> tokenizer config file saved in outputs/checkpoint-4332/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2026] 2023-02-28 18:07:20,992 >> Special tokens file saved in outputs/checkpoint-4332/special_tokens_map.json\n",
            "{'loss': 0.0827, 'learning_rate': 0.0, 'epoch': 5.0}\n",
            "100% 5415/5415 [14:19<00:00,  7.17it/s][INFO|trainer.py:520] 2023-02-28 18:10:11,038 >> The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: sentence1.\n",
            "[INFO|trainer.py:2181] 2023-02-28 18:10:11,040 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2183] 2023-02-28 18:10:11,040 >>   Num examples = 1082\n",
            "[INFO|trainer.py:2186] 2023-02-28 18:10:11,040 >>   Batch size = 8\n",
            "\n",
            "  0% 0/136 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 7/136 [00:00<00:02, 59.05it/s]\u001b[A\n",
            " 10% 13/136 [00:00<00:02, 55.11it/s]\u001b[A\n",
            " 14% 19/136 [00:00<00:02, 55.27it/s]\u001b[A\n",
            " 18% 25/136 [00:00<00:02, 52.01it/s]\u001b[A\n",
            " 23% 31/136 [00:00<00:02, 48.68it/s]\u001b[A\n",
            " 27% 37/136 [00:00<00:01, 50.18it/s]\u001b[A\n",
            " 32% 43/136 [00:00<00:01, 49.70it/s]\u001b[A\n",
            " 36% 49/136 [00:00<00:01, 50.40it/s]\u001b[A\n",
            " 40% 55/136 [00:01<00:01, 49.94it/s]\u001b[A\n",
            " 45% 61/136 [00:01<00:01, 48.00it/s]\u001b[A\n",
            " 49% 67/136 [00:01<00:01, 49.79it/s]\u001b[A\n",
            " 54% 73/136 [00:01<00:01, 47.29it/s]\u001b[A\n",
            " 57% 78/136 [00:01<00:01, 44.26it/s]\u001b[A\n",
            " 61% 83/136 [00:01<00:01, 45.48it/s]\u001b[A\n",
            " 65% 89/136 [00:01<00:00, 47.55it/s]\u001b[A\n",
            " 70% 95/136 [00:01<00:00, 50.05it/s]\u001b[A\n",
            " 74% 101/136 [00:02<00:00, 49.42it/s]\u001b[A\n",
            " 78% 106/136 [00:02<00:00, 49.32it/s]\u001b[A\n",
            " 82% 112/136 [00:02<00:00, 49.52it/s]\u001b[A\n",
            " 87% 118/136 [00:02<00:00, 50.40it/s]\u001b[A\n",
            " 91% 124/136 [00:02<00:00, 48.40it/s]\u001b[A\n",
            " 95% 129/136 [00:02<00:00, 46.49it/s]\u001b[A\n",
            " 99% 134/136 [00:02<00:00, 46.40it/s]\u001b[A02/28/2023 18:10:13 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
            "02/28/2023 18:10:13 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow\n",
            "02/28/2023 18:10:13 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow\n",
            "02/28/2023 18:10:13 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow\n",
            "\n",
            "{'eval_loss': 0.31653064489364624, 'eval_accuracy': 0.9316081330868762, 'eval_precision': 0.9294436580860304, 'eval_recall': 0.934995504361984, 'eval_f1': 0.93104244178035, 'eval_runtime': 2.8565, 'eval_samples_per_second': 378.78, 'eval_steps_per_second': 47.61, 'epoch': 5.0}\n",
            "\n",
            "100% 5415/5415 [14:22<00:00,  7.17it/s]\n",
            "                                     \u001b[A[INFO|trainer.py:1935] 2023-02-28 18:10:13,899 >> Saving model checkpoint to outputs/checkpoint-5415\n",
            "[INFO|configuration_utils.py:391] 2023-02-28 18:10:13,900 >> Configuration saved in outputs/checkpoint-5415/config.json\n",
            "[INFO|modeling_utils.py:1001] 2023-02-28 18:10:15,395 >> Model weights saved in outputs/checkpoint-5415/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2020] 2023-02-28 18:10:15,396 >> tokenizer config file saved in outputs/checkpoint-5415/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2026] 2023-02-28 18:10:15,396 >> Special tokens file saved in outputs/checkpoint-5415/special_tokens_map.json\n",
            "[INFO|trainer.py:1366] 2023-02-28 18:10:19,045 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 867.2895, 'train_samples_per_second': 99.944, 'train_steps_per_second': 6.244, 'train_loss': 0.15144197810940024, 'epoch': 5.0}\n",
            "100% 5415/5415 [14:27<00:00,  6.24it/s]\n",
            "[INFO|trainer.py:1935] 2023-02-28 18:10:19,048 >> Saving model checkpoint to outputs/\n",
            "[INFO|configuration_utils.py:391] 2023-02-28 18:10:19,048 >> Configuration saved in outputs/config.json\n",
            "[INFO|modeling_utils.py:1001] 2023-02-28 18:10:20,683 >> Model weights saved in outputs/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2020] 2023-02-28 18:10:20,683 >> tokenizer config file saved in outputs/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2026] 2023-02-28 18:10:20,684 >> Special tokens file saved in outputs/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        5.0\n",
            "  train_loss               =     0.1514\n",
            "  train_runtime            = 0:14:27.28\n",
            "  train_samples            =      17336\n",
            "  train_samples_per_second =     99.944\n",
            "  train_steps_per_second   =      6.244\n",
            "02/28/2023 18:10:20 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:520] 2023-02-28 18:10:20,729 >> The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: sentence1.\n",
            "[INFO|trainer.py:2181] 2023-02-28 18:10:20,732 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2183] 2023-02-28 18:10:20,732 >>   Num examples = 1082\n",
            "[INFO|trainer.py:2186] 2023-02-28 18:10:20,732 >>   Batch size = 8\n",
            " 98% 133/136 [00:02<00:00, 46.85it/s]02/28/2023 18:10:23 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
            "02/28/2023 18:10:23 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow\n",
            "02/28/2023 18:10:23 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow\n",
            "02/28/2023 18:10:23 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow\n",
            "100% 136/136 [00:02<00:00, 47.26it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        5.0\n",
            "  eval_accuracy           =     0.9316\n",
            "  eval_f1                 =      0.931\n",
            "  eval_loss               =     0.3165\n",
            "  eval_precision          =     0.9294\n",
            "  eval_recall             =      0.935\n",
            "  eval_runtime            = 0:00:02.92\n",
            "  eval_samples            =       1082\n",
            "  eval_samples_per_second =    369.556\n",
            "  eval_steps_per_second   =     46.451\n"
          ]
        }
      ],
      "source": [
        "! python  /content/banglabert/sequence_classification/sequence_classification.py \\\n",
        "    --model_name_or_path \"csebuetnlp/banglabert\" \\\n",
        "    --dataset_dir \"/content/drive/MyDrive/GED Data/data\" \\\n",
        "    --output_dir \"outputs/\" \\\n",
        "    --learning_rate=2e-5 \\\n",
        "    --warmup_ratio 0.1 \\\n",
        "    --gradient_accumulation_steps 2 \\\n",
        "    --weight_decay 0.1 \\\n",
        "    --lr_scheduler_type \"linear\"  \\\n",
        "    --per_device_train_batch_size=8 \\\n",
        "    --per_device_eval_batch_size=8 \\\n",
        "    --max_seq_length 512 \\\n",
        "    --logging_strategy \"epoch\" \\\n",
        "    --save_strategy \"epoch\" \\\n",
        "    --evaluation_strategy \"epoch\" \\\n",
        "    --num_train_epochs=5 --do_train --do_eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIZlYb_2CGpL"
      },
      "outputs": [],
      "source": [
        "# !rm -rf /content/outputs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ! du -sh /content/outputs/checkpoint-1626/*"
      ],
      "metadata": {
        "id": "pQzRzxnex4C-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trying to test the data"
      ],
      "metadata": {
        "id": "AxYYp8bPz27M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! python /content/banglabert/sequence_classification/sequence_classification.py \\\n",
        "    --model_name_or_path \"/content/outputs/checkpoint-2166\" \\\n",
        "    --dataset_dir \"/content/drive/MyDrive/GED Data/data\" \\\n",
        "    --output_dir \"outputs_test/\" \\\n",
        "    --per_device_eval_batch_size=16 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --do_predict"
      ],
      "metadata": {
        "id": "bhE_oznpz2CJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a08a62f-cb03-4238-a9ac-f3144e3487a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-28 18:24:08.008639: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-28 18:24:08.928035: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-28 18:24:08.928150: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-28 18:24:08.928170: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "02/28/2023 18:24:10 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "02/28/2023 18:24:10 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=False,\n",
            "do_predict=True,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=outputs_test/runs/Feb28_18-24-10_5d5c455a313f,\n",
            "logging_first_step=False,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "output_dir=outputs_test/,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=16,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=outputs_test,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=None,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=outputs_test/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "02/28/2023 18:24:10 - WARNING - datasets.builder - Using custom data configuration default-035252c558c77007\n",
            "02/28/2023 18:24:10 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "02/28/2023 18:24:10 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/csv/default-035252c558c77007/0.0.0\n",
            "02/28/2023 18:24:10 - WARNING - datasets.builder - Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-035252c558c77007/0.0.0)\n",
            "02/28/2023 18:24:10 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/csv/default-035252c558c77007/0.0.0\n",
            "100% 3/3 [00:00<00:00, 48.25it/s]\n",
            "[INFO|configuration_utils.py:559] 2023-02-28 18:24:11,054 >> loading configuration file /content/outputs/checkpoint-2166/config.json\n",
            "[INFO|configuration_utils.py:598] 2023-02-28 18:24:11,056 >> Model config ElectraConfig {\n",
            "  \"_name_or_path\": \"csebuetnlp/banglabert\",\n",
            "  \"architectures\": [\n",
            "    \"ElectraForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_size\": 768,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"neg\",\n",
            "    \"1\": \"pos\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"neg\": 0,\n",
            "    \"pos\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"finetuned\": true\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1669] 2023-02-28 18:24:11,058 >> Didn't find file /content/outputs/checkpoint-2166/added_tokens.json. We won't load it.\n",
            "[INFO|tokenization_utils_base.py:1669] 2023-02-28 18:24:11,058 >> Didn't find file /content/outputs/checkpoint-2166/tokenizer.json. We won't load it.\n",
            "[INFO|tokenization_utils_base.py:1737] 2023-02-28 18:24:11,058 >> loading file /content/outputs/checkpoint-2166/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:1737] 2023-02-28 18:24:11,058 >> loading file None\n",
            "[INFO|tokenization_utils_base.py:1737] 2023-02-28 18:24:11,058 >> loading file /content/outputs/checkpoint-2166/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:1737] 2023-02-28 18:24:11,058 >> loading file /content/outputs/checkpoint-2166/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1737] 2023-02-28 18:24:11,058 >> loading file None\n",
            "[INFO|modeling_utils.py:1277] 2023-02-28 18:24:11,167 >> loading weights file /content/outputs/checkpoint-2166/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:1524] 2023-02-28 18:24:14,853 >> All model checkpoint weights were used when initializing ElectraForSequenceClassification.\n",
            "\n",
            "[INFO|modeling_utils.py:1532] 2023-02-28 18:24:14,853 >> All the weights of ElectraForSequenceClassification were initialized from the model checkpoint at /content/outputs/checkpoint-2166.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use ElectraForSequenceClassification for predictions without further training.\n",
            "02/28/2023 18:24:14 - WARNING - datasets.fingerprint - Parameter 'function'=<function main.<locals>.normalize_example at 0x7f1dd70f85e0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "02/28/2023 18:24:14 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-035252c558c77007/0.0.0/cache-1c80317fa3b1799d.arrow\n",
            "02/28/2023 18:24:14 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.normalize_example at 0x7f1dd70f85e0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n",
            "02/28/2023 18:24:14 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-035252c558c77007/0.0.0/cache-bdd640fb06671ad1.arrow\n",
            "02/28/2023 18:24:14 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.normalize_example at 0x7f1dd70f85e0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n",
            "02/28/2023 18:24:14 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-035252c558c77007/0.0.0/cache-3eb13b9046685257.arrow\n",
            "02/28/2023 18:24:14 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.preprocess_function at 0x7f1de543ff70> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n",
            "02/28/2023 18:24:14 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-035252c558c77007/0.0.0/cache-23b8c1e9392456de.arrow\n",
            "02/28/2023 18:24:14 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.preprocess_function at 0x7f1de543ff70> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n",
            "02/28/2023 18:24:14 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-035252c558c77007/0.0.0/cache-1a3d1fa7bc8960a9.arrow\n",
            "02/28/2023 18:24:14 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.preprocess_function at 0x7f1de543ff70> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n",
            "02/28/2023 18:24:14 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-035252c558c77007/0.0.0/cache-bd9c66b3ad3c2d6d.arrow\n",
            "02/28/2023 18:24:16 - INFO - datasets.load - Found main folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/accuracy/accuracy.py at /root/.cache/huggingface/modules/datasets_modules/metrics/accuracy\n",
            "02/28/2023 18:24:16 - INFO - datasets.load - Found specific version folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/accuracy/accuracy.py at /root/.cache/huggingface/modules/datasets_modules/metrics/accuracy/6dba4616f6b2bbd19659d1db3a48cc001c8f13a10cdc73a2641a55f7a60b7b5b\n",
            "02/28/2023 18:24:16 - INFO - datasets.load - Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/accuracy/accuracy.py to /root/.cache/huggingface/modules/datasets_modules/metrics/accuracy/6dba4616f6b2bbd19659d1db3a48cc001c8f13a10cdc73a2641a55f7a60b7b5b/accuracy.py\n",
            "02/28/2023 18:24:16 - INFO - datasets.load - Couldn't find dataset infos file at https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/accuracy/dataset_infos.json\n",
            "02/28/2023 18:24:16 - INFO - datasets.load - Found metadata file for metric https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/accuracy/accuracy.py at /root/.cache/huggingface/modules/datasets_modules/metrics/accuracy/6dba4616f6b2bbd19659d1db3a48cc001c8f13a10cdc73a2641a55f7a60b7b5b/accuracy.json\n",
            "02/28/2023 18:24:18 - INFO - datasets.load - Found main folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/precision/precision.py at /root/.cache/huggingface/modules/datasets_modules/metrics/precision\n",
            "02/28/2023 18:24:18 - INFO - datasets.load - Found specific version folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/precision/precision.py at /root/.cache/huggingface/modules/datasets_modules/metrics/precision/94709a71c6fe37171ef49d3466fec24dee9a79846c9f176dff66a649e9811690\n",
            "02/28/2023 18:24:18 - INFO - datasets.load - Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/precision/precision.py to /root/.cache/huggingface/modules/datasets_modules/metrics/precision/94709a71c6fe37171ef49d3466fec24dee9a79846c9f176dff66a649e9811690/precision.py\n",
            "02/28/2023 18:24:18 - INFO - datasets.load - Couldn't find dataset infos file at https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/precision/dataset_infos.json\n",
            "02/28/2023 18:24:18 - INFO - datasets.load - Found metadata file for metric https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/precision/precision.py at /root/.cache/huggingface/modules/datasets_modules/metrics/precision/94709a71c6fe37171ef49d3466fec24dee9a79846c9f176dff66a649e9811690/precision.json\n",
            "02/28/2023 18:24:19 - INFO - datasets.load - Found main folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/recall/recall.py at /root/.cache/huggingface/modules/datasets_modules/metrics/recall\n",
            "02/28/2023 18:24:19 - INFO - datasets.load - Found specific version folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/recall/recall.py at /root/.cache/huggingface/modules/datasets_modules/metrics/recall/1e3b93e2bed42e1498e628f161d79ee019dd8e78d36985d3c7ecbc018adf35e8\n",
            "02/28/2023 18:24:19 - INFO - datasets.load - Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/recall/recall.py to /root/.cache/huggingface/modules/datasets_modules/metrics/recall/1e3b93e2bed42e1498e628f161d79ee019dd8e78d36985d3c7ecbc018adf35e8/recall.py\n",
            "02/28/2023 18:24:19 - INFO - datasets.load - Couldn't find dataset infos file at https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/recall/dataset_infos.json\n",
            "02/28/2023 18:24:19 - INFO - datasets.load - Found metadata file for metric https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/recall/recall.py at /root/.cache/huggingface/modules/datasets_modules/metrics/recall/1e3b93e2bed42e1498e628f161d79ee019dd8e78d36985d3c7ecbc018adf35e8/recall.json\n",
            "02/28/2023 18:24:21 - INFO - datasets.load - Found main folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/f1/f1.py at /root/.cache/huggingface/modules/datasets_modules/metrics/f1\n",
            "02/28/2023 18:24:21 - INFO - datasets.load - Found specific version folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/f1/f1.py at /root/.cache/huggingface/modules/datasets_modules/metrics/f1/6c86fddbf90432b9c43a8c38c62a0dd9de63bad2ef0a896f9ae20273d6d6f6e9\n",
            "02/28/2023 18:24:21 - INFO - datasets.load - Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/f1/f1.py to /root/.cache/huggingface/modules/datasets_modules/metrics/f1/6c86fddbf90432b9c43a8c38c62a0dd9de63bad2ef0a896f9ae20273d6d6f6e9/f1.py\n",
            "02/28/2023 18:24:21 - INFO - datasets.load - Couldn't find dataset infos file at https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/f1/dataset_infos.json\n",
            "02/28/2023 18:24:21 - INFO - datasets.load - Found metadata file for metric https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/f1/f1.py at /root/.cache/huggingface/modules/datasets_modules/metrics/f1/6c86fddbf90432b9c43a8c38c62a0dd9de63bad2ef0a896f9ae20273d6d6f6e9/f1.json\n",
            "02/28/2023 18:24:24 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:520] 2023-02-28 18:24:24,823 >> The following columns in the test set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: sentence1.\n",
            "[INFO|trainer.py:2181] 2023-02-28 18:24:24,825 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:2183] 2023-02-28 18:24:24,826 >>   Num examples = 3250\n",
            "[INFO|trainer.py:2186] 2023-02-28 18:24:24,826 >>   Batch size = 16\n",
            " 99% 202/204 [00:07<00:00, 24.82it/s]02/28/2023 18:24:32 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
            "02/28/2023 18:24:32 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/precision/default/default_experiment-1-0.arrow\n",
            "02/28/2023 18:24:32 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/recall/default/default_experiment-1-0.arrow\n",
            "02/28/2023 18:24:32 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/f1/default/default_experiment-1-0.arrow\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.9486\n",
            "  predict_f1                 =     0.9482\n",
            "  predict_loss               =     0.1973\n",
            "  predict_precision          =     0.9469\n",
            "  predict_recall             =     0.9501\n",
            "  predict_runtime            = 0:00:07.98\n",
            "  predict_samples_per_second =    407.178\n",
            "  predict_steps_per_second   =     25.558\n",
            "02/28/2023 18:24:32 - INFO - __main__ - ***** Predict results *****\n",
            "100% 204/204 [00:07<00:00, 25.70it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.head(10)"
      ],
      "metadata": {
        "id": "eNhG1N9e1j_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copying model-checkpoints and metrics to drive"
      ],
      "metadata": {
        "id": "dMoNuLjacTzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ! cp -R /content/outputs /content/drive/MyDrive/GED\\ outputs"
      ],
      "metadata": {
        "id": "s8llTFpq2wSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ! cp -R /content/outputs_test /content/drive/MyDrive/GED\\ outputs"
      ],
      "metadata": {
        "id": "3-r_4fik2-S7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ! rm -rf /content/outputs/outputs\n",
        "# ! rm -rf /content/outputs/outputs_test"
      ],
      "metadata": {
        "id": "BEvHtsA6aHp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NER Task (too imbalanced, skipped)"
      ],
      "metadata": {
        "id": "gOX9HMipfnYk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data preprocessing"
      ],
      "metadata": {
        "id": "rcVuMaV6frWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.head(10)\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "-k9wgFuMkkki",
        "outputId": "0dbb8f78-73df-4130-fbe0-cbecbe632a47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text\n",
              "0  সংলাপ শেষে পার্টির সভাপতি রাশেদ খান মেনন ইসিতে...\n",
              "1  দেশটিতে বয়স্কদের মধ্যে ৭১ দশমিক ১ শতাংশ মানুষ...\n",
              "2  প্রতিবেদনে বলা হয়, আত্মসমর্পণের পর শহর থেকে তা...\n",
              "3  তবে কোনও অর্ডারেই ওই যুবক নিজের সঠিক ঠিকানা ব্...\n",
              "4  তিনি বলেন, আমরা যা চাই না তা যাতে না হয় সেটা ক...\n",
              "5  প্রথম শ্রেণির এই টুর্নামেন্টে অপরাজেয় হয়ে অবস্...\n",
              "6  ফলে এখনও হাজার হাজার রোহিঙ্গা জীবন বাঁচাতে বাং...\n",
              "7                        ৩ কাপ পানি দিয়ে দিন পাত্রে।\n",
              "8  আটককৃতরা হলেন, মোহনপুর উপজেলার জাহানাবাদ গ্রাম...\n",
              "9  বেশি বেশি তাল গাছ লাগান, বজ্রপাতে প্রাণহানি কম..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bccbbc72-6b2a-483f-a3e7-3f742257667f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>সংলাপ শেষে পার্টির সভাপতি রাশেদ খান মেনন ইসিতে...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>দেশটিতে বয়স্কদের মধ্যে ৭১ দশমিক ১ শতাংশ মানুষ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>প্রতিবেদনে বলা হয়, আত্মসমর্পণের পর শহর থেকে তা...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>তবে কোনও অর্ডারেই ওই যুবক নিজের সঠিক ঠিকানা ব্...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>তিনি বলেন, আমরা যা চাই না তা যাতে না হয় সেটা ক...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>প্রথম শ্রেণির এই টুর্নামেন্টে অপরাজেয় হয়ে অবস্...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>ফলে এখনও হাজার হাজার রোহিঙ্গা জীবন বাঁচাতে বাং...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>৩ কাপ পানি দিয়ে দিন পাত্রে।</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>আটককৃতরা হলেন, মোহনপুর উপজেলার জাহানাবাদ গ্রাম...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>বেশি বেশি তাল গাছ লাগান, বজ্রপাতে প্রাণহানি কম...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bccbbc72-6b2a-483f-a3e7-3f742257667f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-bccbbc72-6b2a-483f-a3e7-3f742257667f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-bccbbc72-6b2a-483f-a3e7-3f742257667f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset = []\n",
        "# model_path = r'/content/drive/MyDrive/GED Data/bn_pos.pkl'\n",
        "# for index in tqdm(df.index):\n",
        "#     positive = False\n",
        "#     main_pos_tag = bn_pos.tag(model_path, str(df['text'][index]))\n",
        "#     pos_tag = main_pos_tag.copy()\n",
        "#     last_noun_index = -1\n",
        "#     last_verb_index = -1\n",
        "#     last_vaux_index = -1\n",
        "#     last_nv_index = -1\n",
        "#     last_lv_index = -1\n",
        "\n",
        "#     for i in range(len(pos_tag)):\n",
        "#         if pos_tag[i][1] == 'NC':\n",
        "#             last_noun_index = i\n",
        "#         if pos_tag[i][1] == 'VM':\n",
        "#             last_verb_index = i\n",
        "#         if pos_tag[i][1] == 'VAUX':\n",
        "#             last_vaux_index = i\n",
        "#         if pos_tag[i][1] == 'NV':\n",
        "#             last_nv_index = i\n",
        "#         if pos_tag[i][1] == 'LV':\n",
        "#             last_lv_index = i\n",
        "\n",
        "#     if last_noun_index != -1 and last_verb_index != -1:\n",
        "#         pos_tag[last_noun_index], pos_tag[last_verb_index] = pos_tag[last_verb_index], pos_tag[last_noun_index]\n",
        "#         tokens = []\n",
        "#         tags = []\n",
        "#         for i, word in enumerate(pos_tag):\n",
        "#             tokens.append(word[0])\n",
        "#             if i == last_noun_index or i == last_verb_index:\n",
        "#                 tags.append('W')\n",
        "#             else:\n",
        "#                 tags.append('R')\n",
        "\n",
        "#         # dataset.append({\"bn\": wrong_sentence, \"en\": df['text'][index]})\n",
        "\n",
        "#         dataset.append({\"tokens\": tokens, \"tags\": tags})\n",
        "\n",
        "\n",
        "#     closest_vm_to_last_nv = -1\n",
        "#     pos_tag = main_pos_tag.copy()\n",
        "\n",
        "#     if last_nv_index != -1:\n",
        "#         for i in range(last_nv_index, len(pos_tag)):\n",
        "#             if pos_tag[i][1] == \"VM\":\n",
        "#                 closest_vm_to_last_nv = i\n",
        "#                 break\n",
        "\n",
        "#         if closest_vm_to_last_nv != -1:\n",
        "#             pos_tag[last_nv_index], pos_tag[closest_vm_to_last_nv] = pos_tag[closest_vm_to_last_nv], pos_tag[last_nv_index]\n",
        "\n",
        "\n",
        "#             tokens = []\n",
        "#             tags = []\n",
        "#             for i, word in enumerate(pos_tag):\n",
        "\n",
        "#                 tokens.append(word[0])\n",
        "#                 if i == last_nv_index or i == closest_vm_to_last_nv:\n",
        "#                     tags.append('W')\n",
        "#                 else:\n",
        "#                     tags.append('R')\n",
        "#             dataset.append({\"tokens\": tokens, \"tags\": tags})\n",
        "\n",
        "# print(len(dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tWk6bnFgYdZ",
        "outputId": "84542474-6087-4e82-e054-d709e003ba91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20340/20340 [06:20<00:00, 53.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24227\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import random\n",
        "# random.shuffle(dataset)"
      ],
      "metadata": {
        "id": "M35XJQ28lhd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_data = dataset[:int(len(dataset)*0.8)]\n",
        "# valid_data = dataset[int(len(dataset)*0.8) : int(len(dataset)*0.85)]\n",
        "# test_data = dataset[int(len(dataset)*0.85):]\n",
        "# len(train_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqFPQeqilorJ",
        "outputId": "53b9bdf1-beb4-4f5b-84c0-7147e16df28c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19381"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train_df = pd.DataFrame(train_data)\n",
        "# valid_df = pd.DataFrame(valid_data)\n",
        "# test_df = pd.DataFrame(test_data)"
      ],
      "metadata": {
        "id": "b1eH8cERlujx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(f'train_df:{len(train_df)}, valid_df: {len(valid_df)}, test_df: {len(test_df)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nPD9mKmlxTM",
        "outputId": "b718cc63-b4f8-404d-b4e3-545fa7dbf8ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_df:16000, valid_df: 1000, test_df: 3000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# n = 20000\n",
        "# train_df = train_df.sample(int(n*0.8))\n",
        "# valid_df = valid_df.sample(int(n*0.05))\n",
        "# test_df = test_df.sample(int(n*0.15))"
      ],
      "metadata": {
        "id": "k5eghU5gl3_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(train_df.iloc[2]['tokens'])\n",
        "# train_df.iloc[2]['tags']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nM8o9s5hoOBH",
        "outputId": "5c5c9fc4-5337-4966-ef12-6065b9f2de62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['তবে', 'সামাজিক', '-', 'অর্থনৈতিক', '-', 'সাংস্কৃতিক', 'নানান', 'কারণে', 'বার্ধক্যের', 'হয়', 'খুবই', 'কঠিন', 'জীবন', '।']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'W', 'R', 'R', 'W', 'R']"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# with open('data/train.jsonl', 'w', encoding='utf8') as f:\n",
        "#     for item in train_data:\n",
        "#         json.dump(item,f, ensure_ascii=False)\n",
        "#         f.write('\\n')\n",
        "\n",
        "# with open('data/validation.jsonl', 'w', encoding='utf8') as f:\n",
        "#     for item in valid_data:\n",
        "#         json.dump(item,f, ensure_ascii=False)\n",
        "#         f.write('\\n')\n",
        "\n",
        "# with open('data/test.jsonl', 'w', encoding='utf8') as f:\n",
        "#     for item in test_data:\n",
        "#         json.dump(item,f, ensure_ascii=False)\n",
        "#         f.write('\\n')"
      ],
      "metadata": {
        "id": "G4FMvbYKl8aB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ! python /content/banglabert/token_classification/token_classification.py \\\n",
        "#     --model_name_or_path \"csebuetnlp/banglabert\" \\\n",
        "#     --dataset_dir \"data/\" \\\n",
        "#     --output_dir \"outputs/\" \\\n",
        "#     --learning_rate=2e-5 \\\n",
        "#     --warmup_ratio 0.1 \\\n",
        "#     --gradient_accumulation_steps 2 \\\n",
        "#     --weight_decay 0.1 \\\n",
        "#     --lr_scheduler_type \"linear\"  \\\n",
        "#     --per_device_train_batch_size=8 \\\n",
        "#     --per_device_eval_batch_size=8 \\\n",
        "#     --max_seq_length 512 \\\n",
        "#     --logging_strategy \"epoch\" \\\n",
        "#     --save_strategy \"epoch\" \\\n",
        "#     --evaluation_strategy \"epoch\" \\\n",
        "#     --num_train_epochs=3 --do_train --do_eval"
      ],
      "metadata": {
        "id": "f-iQzHVdndkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ! rm -rf /content/outputs"
      ],
      "metadata": {
        "id": "9Te9MG1HqV3G"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}